
<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>2202-CudaProgramming | Chivier&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="1 Requirements">
<meta property="og:type" content="article">
<meta property="og:title" content="2202-CudaProgramming">
<meta property="og:url" content="https://chivier.github.io/2022/02/20/2022/2202-CudaProgramming/index.html">
<meta property="og:site_name" content="Chivier&#39;s Blog">
<meta property="og:description" content="1 Requirements">
<meta property="og:locale">
<meta property="og:image" content="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/graphics/nvcc-options-for-separate-compilation.png">
<meta property="og:image" content="https://chivier.github.io/2022/02/20/2022/2202-CudaProgramming/image1.png">
<meta property="og:image" content="https://chivier.github.io/2022/02/20/2022/2202-CudaProgramming/image2.png">
<meta property="og:image" content="https://chivier.github.io/2022/02/20/2022/2202-CudaProgramming/image3.png">
<meta property="og:image" content="https://chivier.github.io/2022/02/20/2022/2202-CudaProgramming/image4.png">
<meta property="og:image" content="https://chivier.github.io/2022/02/20/2022/2202-CudaProgramming/image5.png">
<meta property="article:published_time" content="2022-02-19T22:28:54.000Z">
<meta property="article:modified_time" content="2022-02-20T06:55:45.791Z">
<meta property="article:author" content="Chivier Humber">
<meta property="article:tag" content="Develop">
<meta property="article:tag" content="Cuda">
<meta property="article:tag" content="Parallel">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/graphics/nvcc-options-for-separate-compilation.png">
  
    <link rel="alternative" href="https://github.com/Chivier" title="Chivier&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
<link rel="stylesheet" href="/css/style.css">

  <!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->
  
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"></head>
<body>
<div id="container">
  <div id="wrap">
    <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Chivier&#39;s Blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/Whoami">About</a>
        
          <a class="main-nav-link" href="/categories">Catagories</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" target="_blank" rel="noopener" href="https://github.com/Chivier" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//www.baidu.com/baidu" method="get" accept-charset="utf-8" class="search-form">
          <input type="search" name="word" maxlength="20" class="search-form-input" placeholder="Search">
          <input type="submit" value="" class="search-form-submit">
          <input name=tn type=hidden value="bds">
          <input name=cl type=hidden value="3">
          <input name=ct type=hidden value="2097152">
          <input type="hidden" name="si" value="chivier.github.io">
        </form>
      </div>
    </div>
  </div>
</header>
    <div class="outer">
      <section id="main"><article id="[post]-2022/2202-CudaProgramming" class="article article-type-[post]" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2022/02/20/2022/2202-CudaProgramming/" class="article-date">
  <time datetime="2022-02-19T22:28:54.000Z" itemprop="datePublished">2022-02-20</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Cuda/">Cuda</a>►<a class="article-category-link" href="/categories/Cuda/Parallel/">Parallel</a>►<a class="article-category-link" href="/categories/Cuda/Parallel/Develop/">Develop</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      2202-CudaProgramming
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
      <!-- Table of Contents -->
      
      <div id="toc" class="toc-article">
        <strong class="toc-title">目录</strong>
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-Requirements"><span class="toc-text">1 Requirements</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-%E8%AE%BE%E5%A4%87%E5%92%8C%E4%B8%BB%E6%9C%BA"><span class="toc-text">2 设备和主机</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-%E8%AE%BE%E5%A4%87%E5%AE%9A%E4%B9%89"><span class="toc-text">2.1 设备定义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-%E5%87%BD%E6%95%B0%E8%B0%83%E7%94%A8"><span class="toc-text">2.2 函数调用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-%E5%87%BD%E6%95%B0%E9%99%90%E5%88%B6"><span class="toc-text">2.3 函数限制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4-Cuda-Version-amp-GPU-Version"><span class="toc-text">2.4 Cuda Version &amp; GPU Version</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-5-%E5%85%B3%E4%BA%8E%E8%AE%BE%E5%A4%87%E4%B8%8A%E7%9A%84%E8%BE%93%E5%87%BA%E9%97%AE%E9%A2%98"><span class="toc-text">2.5 关于设备上的输出问题</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-Block-%E5%92%8C-Thread"><span class="toc-text">3 Block 和 Thread</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-%E4%BB%8E%E4%BE%8B%E5%AD%90%E5%BC%80%E5%A7%8B"><span class="toc-text">3.1 从例子开始</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-Block-%E5%92%8C-Thread-%E7%9A%84%E8%AE%BE%E8%AE%A1%E6%84%8F%E4%B9%89"><span class="toc-text">3.2 Block 和 Thread 的设计意义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-Flatten-Mode-%E5%92%8C-Dimension-Mode"><span class="toc-text">3.3 Flatten Mode 和 Dimension Mode</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4-%E5%A6%82%E4%BD%95%E8%AE%BE%E7%BD%AE%E8%AE%A1%E7%AE%97%E8%B5%84%E6%BA%90%EF%BC%9F"><span class="toc-text">3.4 如何设置计算资源？</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-%E5%8F%98%E9%87%8F%E8%A1%8C%E4%B8%BA"><span class="toc-text">4 变量行为</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-%E6%98%BE%E5%AD%98%E5%92%8C%E5%86%85%E5%AD%98"><span class="toc-text">4.1 显存和内存</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-%E6%98%BE%E5%AD%98%E5%86%85%E5%AD%98%E4%B8%8D%E8%83%BD%E4%BA%92%E7%9B%B8%E8%B0%83%E7%94%A8"><span class="toc-text">4.2 显存内存不能互相调用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-1-%E8%AE%BE%E5%A4%87%E8%B0%83%E7%94%A8%E5%86%85%E5%AD%98%E5%A4%B1%E8%B4%A5"><span class="toc-text">4.2.1 设备调用内存失败</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-2-%E4%B8%BB%E6%9C%BA%E8%B0%83%E7%94%A8%E6%98%BE%E5%AD%98%E5%A4%B1%E8%B4%A5"><span class="toc-text">4.2.2 主机调用显存失败</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-3-cuda-Error-Handling"><span class="toc-text">4.2.3 cuda Error Handling</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-5-cudaMemcpy"><span class="toc-text">4.2.5 cudaMemcpy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-4-Unified-Memory"><span class="toc-text">4.2.4 Unified Memory</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3-%E5%8F%98%E9%87%8F%E4%BC%A0%E8%BE%93%E5%92%8C%E8%AE%A1%E7%AE%97"><span class="toc-text">4.3 变量传输和计算</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-1-grid-stride-loop"><span class="toc-text">4.3.1 grid-stride loop</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-2-%E7%AE%97%E5%AD%90%E4%BC%A0%E8%BE%93"><span class="toc-text">4.3.2 算子传输</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-4-%E9%AB%98%E7%BA%A7%E8%AE%A1%E7%AE%97%E8%A1%8C%E4%B8%BA"><span class="toc-text">4.4 高级计算行为</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-1-thrust"><span class="toc-text">4.4.1 thrust</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-1-1-vector-%E5%AF%B9%E6%A0%87"><span class="toc-text">4.4.1.1 vector 对标</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-4-1-2-other-thrusts"><span class="toc-text">4.4.1.2 other thrusts</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-2-atomic"><span class="toc-text">4.4.2 atomic</span></a></li></ol></li></ol></li></ol>
      </div>
      
        <h1 id="1-Requirements"><a href="#1-Requirements" class="headerlink" title="1 Requirements"></a>1 Requirements</h1><span id="more"></span>


<p>在开始教程之前，简单说明一下下面步骤的需求和测试方法：</p>
<ol>
<li>Git: 之后的范例使用 git 作为拉取</li>
<li>Cuda 11+：使用 11 以上的 cuda 版本确保之后的步骤可以正常使用</li>
<li>CMake：项目构建采用 cmake 确保多平台可以进行测试和实验</li>
</ol>
<p>官方文档的学习曲线比较陡峭，下面整理一些例子帮助快速上手。<br>参考官方手册：Cuda Programming Guide，见附件。此后简称它“手册”。</p>
<h1 id="2-设备和主机"><a href="#2-设备和主机" class="headerlink" title="2 设备和主机"></a>2 设备和主机</h1><p>首先在这里，我个人想赞美一下 Nvidia 作为国际一流大厂的开源精神和优秀兼容性。现在的 Cuda 支持跨平台、语法兼容。这里语法兼容是指：cuda 是包含 C++17 语法的。直接写的 C++ 代码是可以在机器上直接运行的。</p>
<p>官方也有一个例子，说明 cuda 和 g++的编译内容可以混合使用：<br><img src="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/graphics/nvcc-options-for-separate-compilation.png" alt="Flow diagram of nvcc options for separate compilation"><br>这一设计大大节约了程序移植的时间。</p>
<p>在开始编程之前，需要分离我们的传统思维，程序可以不只在 CPU 上运行，还可以在 GPU 上面运行。因此，这里诞生两个概念：Host &amp; Device。<br>实际上，对于大部分的异构计算框架，例如：OpenCL、UPC 等等，都是可以指定 Host &amp; Device 的。这里的 Host 一般代指我们的 CPU, 而 Device 代指我们的 GPU。<br>下面是一个例子：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">  </span><br><span class="line"><span class="function">__device__ <span class="keyword">void</span> <span class="title">gpu_hello</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">   <span class="built_in">printf</span>(<span class="string">&quot;gpu hello!\n&quot;</span>);  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function">__host__ <span class="keyword">void</span> <span class="title">cpu_hello</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">   <span class="built_in">printf</span>(<span class="string">&quot;cpu hello!\n&quot;</span>);  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">kernel</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">   gpu_hello();  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">   kernel&lt;&lt;&lt;<span class="number">1</span>, <span class="number">2</span>&gt;&gt;&gt;();  </span><br><span class="line">   cudaDeviceSynchronize();  </span><br><span class="line">   cpu_hello();  </span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其中我们可以看到三类不同的函数修饰：<br>_<em>device_<em>，__host__，和 __global\</em></em></p>
<p>在手册 4.2.1 函数类型限定词一节中进行了详细介绍。</p>
<h2 id="2-1-设备定义"><a href="#2-1-设备定义" class="headerlink" title="2.1 设备定义"></a>2.1 设备定义</h2><ul>
<li>_<em>device_</em> 是在设备上跑的，<strong>只可以</strong>从设备上调用</li>
<li>_<em>host_</em> 是在 CPU 上跑的，<strong>只可以</strong>从主机上调用</li>
<li>_<em>global_</em> 是在设备商跑的，<strong>只可以</strong>从主机上调用</li>
</ul>
<img src="image1.png" width="80%" height="80%">

<p>如果一个函数不加修饰，默认他是 _<em>device_</em> 函数，正如上面的 main 一样。</p>
<p>如果一个函数需要同时在 CPU 和 GPU 上都能执行，那么可以同时加上 host 和 device 两个关键字。<br>需要判断具体在那个设备上，可以使用：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// cuda</span></span><br><span class="line"><span class="comment">// cpu</span></span><br></pre></td></tr></table></figure>

<h2 id="2-2-函数调用"><a href="#2-2-函数调用" class="headerlink" title="2.2 函数调用"></a>2.2 函数调用</h2><p>如果是 device 或者 host 函数，我们可以在恰当的位置直接调用。<br>如果是 global 调用的时候，我们需要用&lt;&lt;&lt;arg1, arg2&gt;&gt;&gt;，去声明我们申请的资源。这个我们在[下一节](#\ 3\ Block\ 和\ Thread) 会说明。</p>
<h2 id="2-3-函数限制"><a href="#2-3-函数限制" class="headerlink" title="2.3 函数限制"></a>2.3 函数限制</h2><p>由于计算的行为限制，一些特殊的程序行为在 cuda 代码中是被严格禁止的，例如：</p>
<ol>
<li>_<em>host_</em> 和 _<em>global_</em> 不支持递归</li>
<li>_<em>global_</em> 返回值要求是 void</li>
<li>调用 GPU 的函数声明和定义不要分离，写在同一个文件里</li>
</ol>
<p>更多限制见手册，不过上述两条基本包含了日常开发的需要。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这里额外补充一点，是因为特殊的需要说明一下。</span><br><span class="line">关于上述的第三点，我们尽管可以使用特殊的方法，例如CUDA_SEPARABLE_COMPILATION等方法可以分离定义，但是会有无法内联等问题对性能产生巨大影响。处于这些原因，我个人建议__global__和__device__ **&lt;u&gt;声明和调用他们的地方&lt;&#x2F;u&gt;** 最好写到同一个文件里。</span><br></pre></td></tr></table></figure>

<h2 id="2-4-Cuda-Version-amp-GPU-Version"><a href="#2-4-Cuda-Version-amp-GPU-Version" class="headerlink" title="2.4 Cuda Version &amp; GPU Version"></a>2.4 Cuda Version &amp; GPU Version</h2><p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/CUDA">https://en.wikipedia.org/wiki/CUDA</a></p>
<p>这个部分一般来说不太需要关心，但是上面给出一个链接用来说明各个版本的 GPU 的 compute capacity 的问题，每张 GPU 都有一个属于自己的版本号，版本号是向下兼容的，即高版本号的卡可以跑任何版本号低于自己卡上编译出来的程序。用尽可能高版本的卡的特性去编译程序有利于充分发挥程序的性能。</p>
<p>在 <code>CMakeLists.txt</code> 中设置：</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> (CMAKE_CUDA_ARCHITECTURES <span class="number">61</span>)</span><br></pre></td></tr></table></figure>

<p>其中 61 是 1080 的版本号，在表格中有写出，也可以使用程序输出：</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">  </span><br><span class="line">__device__ void gpu_hello() &#123;  </span><br><span class="line">   <span class="comment">#ifdef __CUDA_ARCH__  </span></span><br><span class="line">       printf(<span class="string">&quot;%d\n&quot;</span>, __CUDA_ARCH__);  </span><br><span class="line">   <span class="comment">#endif  </span></span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line">__global__ void kernel() &#123;  </span><br><span class="line">   gpu_hello();  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line">int main() &#123;  </span><br><span class="line">   kernel&lt;&lt;&lt;<span class="number">1</span>, <span class="number">1</span>&gt;&gt;&gt;();  </span><br><span class="line">   cudaDeviceSynchronize();  </span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>用来检测当前的版本是否正确，因为如果在 <code>CMakeLists.txt</code> 中不指明，那么机器会默认使用最低版本的驱动，也就是 520 版本的进行编译和后续优化。这显然不是我们希望的。</p>
<p>这里有一个我之前收藏的小文，介绍了更多的硬件细节：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/394352476">https://zhuanlan.zhihu.com/p/394352476</a></p>
<h2 id="2-5-关于设备上的输出问题"><a href="#2-5-关于设备上的输出问题" class="headerlink" title="2.5 关于设备上的输出问题"></a>2.5 关于设备上的输出问题</h2><p>如果我们在 device 或者 global 的代码里调用了 printf 等输出函数，这个时候需要使用前面已经多次使用过的 <code>cudaDeviceSynchronize()</code> 才能生效。因为处于高性能的需求，我们的 CPU 代码执行和 GPU 代码执行是异步执行的。所以我们调用 GPU 代码之后，CPU 程序实际上是会继续执行的。如果要等上面的 GPU 代码执行完成，需要在此处同步等待一下。<br>（可以尝试把之前程序的 <code>cudaDeviceSynchronize()</code> 删除进行一个小小的测试）<br>此外 cout 等流输出在 cuda 中是禁止的，因为流输出里包含了过多的函数行为和复杂特性，Nvidia 的工程师暂时还没有实现这些特性。</p>
<h1 id="3-Block-和-Thread"><a href="#3-Block-和-Thread" class="headerlink" title="3 Block 和 Thread"></a>3 Block 和 Thread</h1><p>上面一节我们提到，如果我们调用 global 声明的函数的时候需要说明计算资源。这里我们解释一下 &lt;&lt;&lt;arg1, arg2&gt;&gt;&gt; 中两个参数的含义。</p>
<h2 id="3-1-从例子开始"><a href="#3-1-从例子开始" class="headerlink" title="3.1 从例子开始"></a>3.1 从例子开始</h2><p>首先先从下面这个例子说明：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">  </span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;  </span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">kernel</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">   <span class="built_in">printf</span>(<span class="string">&quot;Block %d of %d, Thread %d of %d\n&quot;</span>, blockIdx.x, gridDim.x, threadIdx.x, blockDim.x);  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">   kernel&lt;&lt;&lt;<span class="number">4</span>, <span class="number">3</span>&gt;&gt;&gt;();  </span><br><span class="line">   cudaDeviceSynchronize();  </span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Block 0 of 4, Thread 0 of 3  </span><br><span class="line">Block 0 of 4, Thread 1 of 3  </span><br><span class="line">Block 0 of 4, Thread 2 of 3  </span><br><span class="line">Block 2 of 4, Thread 0 of 3  </span><br><span class="line">Block 2 of 4, Thread 1 of 3  </span><br><span class="line">Block 2 of 4, Thread 2 of 3  </span><br><span class="line">Block 1 of 4, Thread 0 of 3  </span><br><span class="line">Block 1 of 4, Thread 1 of 3  </span><br><span class="line">Block 1 of 4, Thread 2 of 3  </span><br><span class="line">Block 3 of 4, Thread 0 of 3  </span><br><span class="line">Block 3 of 4, Thread 1 of 3  </span><br><span class="line">Block 3 of 4, Thread 2 of 3</span><br></pre></td></tr></table></figure>

<p>Cuda 有两个层级的并行，一个是 block 级别，一个是 thread 级别。<br>&lt;&lt;&lt;arg1, arg2&gt;&gt;&gt; 中：</p>
<ul>
<li>Arg1 = block 数目 = <code>gridDim.x</code></li>
<li>Arg2 = 每个 block 中 thread 的数目 = <code>blockDim.x</code><br>（我曾经反对首字母小写的驼峰命名，后来被 Nvidia 教育了）</li>
</ul>
<h2 id="3-2-Block-和-Thread-的设计意义"><a href="#3-2-Block-和-Thread-的设计意义" class="headerlink" title="3.2 Block 和 Thread 的设计意义"></a>3.2 Block 和 Thread 的设计意义</h2><p>为什么需要进行这两级的设计呢？<br>其实这个和 GPU 的硬件设计方式有关系。我们的代码直接和硬件设计相关。</p>
<p>首先从命名上不难看出，GPU 的设计是为了进行 2D 和 3D 的网格类型计算。对应的应用就是：图形图像处理、粒子引擎模拟、网格算法计算。我们在计算是经常需要 “切分大数据，进行分发计算，最后再进行收集”。也就是常说的 Scatter-and-Gather, 或者 Fork-and-Merge。</p>
<p>现在的 GPU 架构中：</p>
<ul>
<li>一个 GPU = 多个 Streaming Multiprocessor (SM) + cache 组成</li>
<li>一个 SM = Streaming Processor（SP）+ cache 组成</li>
<li>SM 用于处理 block</li>
<li>SP 用于处理 thread</li>
</ul>
<p>很大一方面理由就是为了更加充分利用多级缓存的优势。</p>
<h2 id="3-3-Flatten-Mode-和-Dimension-Mode"><a href="#3-3-Flatten-Mode-和-Dimension-Mode" class="headerlink" title="3.3 Flatten Mode 和 Dimension Mode"></a>3.3 Flatten Mode 和 Dimension Mode</h2><p>由于 Block 和 Thread 的两级设计导致我们的划分可能存在一定的困难，有一个简单粗暴的方法可以直接把他们压平。直接把上面那个例子进行压平改写：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">  </span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;  </span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">kernel</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">   <span class="built_in">printf</span>(<span class="string">&quot;Thread %d of %d\n&quot;</span>, blockIdx.x * blockDim.x + threadIdx.x, blockDim.x * gridDim.x);  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">   kernel&lt;&lt;&lt;<span class="number">4</span>, <span class="number">3</span>&gt;&gt;&gt;();  </span><br><span class="line">   cudaDeviceSynchronize();  </span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Thread 0 of 12  </span><br><span class="line">Thread 1 of 12  </span><br><span class="line">Thread 2 of 12  </span><br><span class="line">Thread 6 of 12  </span><br><span class="line">Thread 7 of 12  </span><br><span class="line">Thread 8 of 12  </span><br><span class="line">Thread 3 of 12  </span><br><span class="line">Thread 4 of 12  </span><br><span class="line">Thread 5 of 12  </span><br><span class="line">Thread 9 of 12  </span><br><span class="line">Thread 10 of 12  </span><br><span class="line">Thread 11 of 12</span><br></pre></td></tr></table></figure>

<p>由于 cuda 的适用范围，还有另外一种模式的其实在 HPC 应用中更加受到欢迎和广泛使用，一般称为 Dimension Mode。</p>
<p>首先展示一下结果：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">  </span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">kernel</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">   <span class="built_in">printf</span>(<span class="string">&quot;Block (%d,%d,%d) of (%d,%d,%d), Thread (%d,%d,%d) of (%d,%d,%d)\n&quot;</span>,  </span><br><span class="line">          blockIdx.x, blockIdx.y, blockIdx.z,  </span><br><span class="line">          gridDim.x, gridDim.y, gridDim.z,  </span><br><span class="line">          threadIdx.x, threadIdx.y, threadIdx.z,  </span><br><span class="line">          blockDim.x, blockDim.y, blockDim.z);  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">   kernel&lt;&lt;&lt;dim3(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>), dim3(<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>)&gt;&gt;&gt;();  </span><br><span class="line">   cudaDeviceSynchronize();  </span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>首先看三维的例子：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Block (0,0,0) of (2,1,1), Thread (0,0,0) of (2,2,2)  </span><br><span class="line">Block (0,0,0) of (2,1,1), Thread (1,0,0) of (2,2,2)  </span><br><span class="line">Block (0,0,0) of (2,1,1), Thread (0,1,0) of (2,2,2)  </span><br><span class="line">Block (0,0,0) of (2,1,1), Thread (1,1,0) of (2,2,2)  </span><br><span class="line">Block (0,0,0) of (2,1,1), Thread (0,0,1) of (2,2,2)  </span><br><span class="line">Block (0,0,0) of (2,1,1), Thread (1,0,1) of (2,2,2)  </span><br><span class="line">Block (0,0,0) of (2,1,1), Thread (0,1,1) of (2,2,2)  </span><br><span class="line">Block (0,0,0) of (2,1,1), Thread (1,1,1) of (2,2,2)  </span><br><span class="line">Block (1,0,0) of (2,1,1), Thread (0,0,0) of (2,2,2)  </span><br><span class="line">Block (1,0,0) of (2,1,1), Thread (1,0,0) of (2,2,2)  </span><br><span class="line">Block (1,0,0) of (2,1,1), Thread (0,1,0) of (2,2,2)  </span><br><span class="line">Block (1,0,0) of (2,1,1), Thread (1,1,0) of (2,2,2)  </span><br><span class="line">Block (1,0,0) of (2,1,1), Thread (0,0,1) of (2,2,2)  </span><br><span class="line">Block (1,0,0) of (2,1,1), Thread (1,0,1) of (2,2,2)  </span><br><span class="line">Block (1,0,0) of (2,1,1), Thread (0,1,1) of (2,2,2)  </span><br><span class="line">Block (1,0,0) of (2,1,1), Thread (1,1,1) of (2,2,2)</span><br></pre></td></tr></table></figure>

<p>再来给出一个二维的例子：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">  </span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">kernel</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">   <span class="built_in">printf</span>(<span class="string">&quot;Block (%d,%d,%d) of (%d,%d,%d), Thread (%d,%d,%d) of (%d,%d,%d)\n&quot;</span>,  </span><br><span class="line">          blockIdx.x, blockIdx.y, blockIdx.z,  </span><br><span class="line">          gridDim.x, gridDim.y, gridDim.z,  </span><br><span class="line">          threadIdx.x, threadIdx.y, threadIdx.z,  </span><br><span class="line">          blockDim.x, blockDim.y, blockDim.z);  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">   kernel&lt;&lt;&lt;dim3(<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>), dim3(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)&gt;&gt;&gt;();  </span><br><span class="line">   cudaDeviceSynchronize();  </span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>结果最后一个维度都是 0, 我们使用结果的时候不使用 z 维度即可</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Block (1,2,0) of (2,3,1), Thread (0,0,0) of (2,1,1)  </span><br><span class="line">Block (1,2,0) of (2,3,1), Thread (1,0,0) of (2,1,1)  </span><br><span class="line">Block (0,2,0) of (2,3,1), Thread (0,0,0) of (2,1,1)  </span><br><span class="line">Block (0,2,0) of (2,3,1), Thread (1,0,0) of (2,1,1)  </span><br><span class="line">Block (0,1,0) of (2,3,1), Thread (0,0,0) of (2,1,1)  </span><br><span class="line">Block (0,1,0) of (2,3,1), Thread (1,0,0) of (2,1,1)  </span><br><span class="line">Block (1,0,0) of (2,3,1), Thread (0,0,0) of (2,1,1)  </span><br><span class="line">Block (1,0,0) of (2,3,1), Thread (1,0,0) of (2,1,1)  </span><br><span class="line">Block (0,0,0) of (2,3,1), Thread (0,0,0) of (2,1,1)  </span><br><span class="line">Block (0,0,0) of (2,3,1), Thread (1,0,0) of (2,1,1)  </span><br><span class="line">Block (1,1,0) of (2,3,1), Thread (0,0,0) of (2,1,1)  </span><br><span class="line">Block (1,1,0) of (2,3,1), Thread (1,0,0) of (2,1,1)</span><br></pre></td></tr></table></figure>

<h2 id="3-4-如何设置计算资源？"><a href="#3-4-如何设置计算资源？" class="headerlink" title="3.4 如何设置计算资源？"></a>3.4 如何设置计算资源？</h2><p>上面说了我们 GPU 并行的时候有两个层级，第一个是 Block 级别，第二个是 Thread 级别。为此我们需要设置两个级别的数目。</p>
<p>首先我们先看我们最多可以用多少资源。使用 cuda samples 中的 <a target="_blank" rel="noopener" href="https://github.com/nvidia/cuda-samples">deviceQuery</a> （Make 直接编译整个 cuda-samples 项目可以找到）</p>
<img src="image2.png" width="80%" height="80%">

<p>此程序会列举机器上的所有设备，列举设备之后可以展示出机器的所有详细信息。这里我们看到了关键的几行信息：</p>
<p>Maximum number of threads per block<br>以及<br>Maximum dimension size of a thread block</p>
<p>前者比较好理解，我们看看后一个 dimension 的含义，首先先列举一下<a target="_blank" rel="noopener" href="https://forums.developer.nvidia.com/t/maximum-number-of-threads-on-thread-block/46392/2">官方论坛</a>的说辞：</p>
<blockquote>
<p>There are multiple limits. All must be satisfied.</p>
<ol>
<li> The maximum number of threads in the block is limited to 1024. This is the product of whatever your threadblock dimensions are (x_y_z). For example (32,32,1) creates a block of 1024 threads. (33,32,1) is not legal, since <code>33*32*1</code> &gt; 1024.</li>
<li> The maximum x-dimension is 1024. (1024, 1, 1) is legal. (1025, 1, 1) is not legal.</li>
<li> The maximum y-dimension is 1024. (1, 1024, 1) is legal. (1, 1025, 1) is not legal.</li>
<li> The maximum z-dimension is 64. (1, 1, 64) is legal. (2, 2, 64) is also legal. (1, 1, 65) is not legal.</li>
</ol>
</blockquote>
<p>配合论坛中的内容，这里也很好理解了。</p>
<p>结合了上面的例子，cuda 的变成已经变得相对直观，用来优化一些简单的循环已经可以实现了。这里可以概括一下：</p>
<ol>
<li>cuda Block 级别相当于 C++ 线程，数目可以设置比较大，调度依靠 GPU ，方式类似于 CPU 调度 threads</li>
<li>cuda Thread 级别相当于 SIMD，有数目上限，受限于 cuda core 的数目和一些维度参数</li>
</ol>
<h1 id="4-变量行为"><a href="#4-变量行为" class="headerlink" title="4 变量行为"></a>4 变量行为</h1><p>简单的例子说明完成，下面开始介绍每一个具体的函数内部，我们程序的行为。</p>
<h2 id="4-1-显存和内存"><a href="#4-1-显存和内存" class="headerlink" title="4.1 显存和内存"></a>4.1 显存和内存</h2><p>在使用 GPU 编程（也有可能在玩游戏）的时候，有一个词时常出现：显存。<br>这个词已经暗示我们 GPU 的内存分布和 CPU 的内存分布实际上是不一样的。我们在设备上的内容、和主机上的内容直接需要一些同步机制去同步。</p>
<p>首先一图概览接下来的内容：</p>
<img src="image3.png" width="80%" height="80%">

<p>简述一下：</p>
<ol>
<li>显存和内存独立管理各自的 DRAM</li>
<li>互相调用树枝需要使用 cudaMemcpy</li>
<li>Unified 统一管理模式可以解决管理问题，但是代价是需要额外的时间开销</li>
</ol>
<p>下面配合具体的例子说明。</p>
<h2 id="4-2-显存内存不能互相调用"><a href="#4-2-显存内存不能互相调用" class="headerlink" title="4.2 显存内存不能互相调用"></a>4.2 显存内存不能互相调用</h2><h3 id="4-2-1-设备调用内存失败"><a href="#4-2-1-设备调用内存失败" class="headerlink" title="4.2.1 设备调用内存失败"></a>4.2.1 设备调用内存失败</h3><p>我们复习一下在内存中如何申请空间，在 C/C++ 中，我们一般会使用一对函数： malloc &amp; free 用于一块内存的申请与释放。</p>
<p>下面给出一个例子：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">  </span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">kernel</span><span class="params">(<span class="keyword">int</span> *arr)</span> </span>&#123;  </span><br><span class="line">   arr[<span class="number">0</span>] = <span class="number">0</span>;  </span><br><span class="line">   <span class="keyword">int</span> index = <span class="number">1</span>;  </span><br><span class="line">   <span class="keyword">while</span> (arr[index] != <span class="number">0</span>) &#123;  </span><br><span class="line">       arr[<span class="number">0</span>] += arr[index];  </span><br><span class="line">       index++;  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">   <span class="keyword">int</span> *a;  </span><br><span class="line">   a = (<span class="keyword">int</span> *)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="keyword">int</span>) * <span class="number">12</span>);  </span><br><span class="line">   <span class="keyword">int</span> index = <span class="number">1</span>;  </span><br><span class="line">   <span class="keyword">for</span> (index = <span class="number">1</span>; index &lt;= <span class="number">10</span>; ++index) &#123;  </span><br><span class="line">       a[index] = index;  </span><br><span class="line">   &#125;  </span><br><span class="line">   kernel&lt;&lt;&lt;<span class="number">1</span>, <span class="number">1</span>&gt;&gt;&gt;(a);  </span><br><span class="line">   <span class="built_in">printf</span>(<span class="string">&quot;%d&quot;</span>, a[<span class="number">0</span>]);  </span><br><span class="line">   cudaDeviceSynchronize();  </span><br><span class="line">   <span class="built_in">free</span>(a);</span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="4-2-2-主机调用显存失败"><a href="#4-2-2-主机调用显存失败" class="headerlink" title="4.2.2 主机调用显存失败"></a>4.2.2 主机调用显存失败</h3><p>这里首先说明一下 cuda 是如何展示显存的。<br>在介绍之前，先需要介绍一下两个函数接口：cudaMalloc &amp; cudaFree</p>
<p>这里需要说明的是 cudaMalloc 的用法和 malloc 差异极大：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">cudaError_t <span class="title">cudaMalloc</span> <span class="params">(<span class="keyword">void</span>** ptr, <span class="keyword">size_t</span> size)</span></span>;</span><br></pre></td></tr></table></figure>

<p>配合下图理解：</p>
<img src="image4.png" width="80%" height="80%">

<p>我们申请的是一个 “指向 GPU 数据指针的指针”。</p>
<p>我们申请到的空间是在一个二级指针上存储，下面举一个例子：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">  </span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">kernel</span><span class="params">(<span class="keyword">int</span> *arr)</span> </span>&#123;  </span><br><span class="line">   arr[<span class="number">0</span>] = <span class="number">0</span>;  </span><br><span class="line">   <span class="keyword">int</span> index = <span class="number">1</span>;  </span><br><span class="line">   <span class="keyword">while</span> (arr[index] != <span class="number">0</span>) &#123;  </span><br><span class="line">       arr[<span class="number">0</span>] += arr[index];  </span><br><span class="line">       index++;  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">   <span class="keyword">int</span> *a;  </span><br><span class="line">   cudaMalloc(&amp;a, <span class="keyword">sizeof</span>(<span class="keyword">int</span>) * <span class="number">12</span>);  </span><br><span class="line">   <span class="keyword">int</span> index = <span class="number">1</span>;  </span><br><span class="line">   <span class="keyword">for</span> (index = <span class="number">1</span>; index &lt;= <span class="number">10</span>; ++index) &#123;  </span><br><span class="line">       a[index] = index;  </span><br><span class="line">   &#125;  </span><br><span class="line">   kernel&lt;&lt;&lt;<span class="number">1</span>, <span class="number">1</span>&gt;&gt;&gt;(a);  </span><br><span class="line">   <span class="built_in">printf</span>(<span class="string">&quot;%d&quot;</span>, a[<span class="number">0</span>]);  </span><br><span class="line">   cudaDeviceSynchronize();  </span><br><span class="line">   cudaFree(a);</span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="4-2-3-cuda-Error-Handling"><a href="#4-2-3-cuda-Error-Handling" class="headerlink" title="4.2.3 cuda Error Handling"></a>4.2.3 cuda Error Handling</h3><p>上面两个失败的例子再次说明了内存和显存不能“直接”混合使用。（下面会说一种支持混合使用的技术）<br>前者结果是 0, 后者结果是段错误。导致的原因是因为内存地址和显存地址不是直接统一的。</p>
<p>为了方便调试，cuda 提供了一系列的纠错调试机制，在前面的程序中，用到的：<code>cudaMalloc</code>，<code>cudaFree</code>，<code>cudaDeviceSynchronize</code> 都有返回类型便于我们调试，返回类型是一个 <code>cudaError_t</code> 的没枚举类型，可以被 printf 直接输出。</p>
<p>但是输出了我们是不能理解函数的具体含义的，也不方便我们进行 debug。这里 cuda samples 里面提供了一个非常好的例子，我们可以直接引用 <code>helper_cuda.h</code> 头文件解决问题。</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.10</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">set</span>(CMAKE_CXX_STANDARD <span class="number">17</span>)  </span><br><span class="line"><span class="keyword">set</span>(CMAKE_BUILD_TYPE Release)  </span><br><span class="line"><span class="keyword">set</span>(CMAKE_CUDA_ARCHITECTURES <span class="number">61</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">project</span>(devices LANGUAGES CXX CUDA)  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">add_executable</span>(hello hello.cu)  </span><br><span class="line"><span class="keyword">target_include_directories</span>(hello PUBLIC /usr/local/cuda/samples/common/inc)</span><br></pre></td></tr></table></figure>

<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">  </span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">kernel</span><span class="params">(<span class="keyword">int</span> *arr)</span> </span>&#123;  </span><br><span class="line">   arr[<span class="number">0</span>] = <span class="number">0</span>;  </span><br><span class="line">   <span class="keyword">int</span> index = <span class="number">1</span>;  </span><br><span class="line">   <span class="keyword">while</span> (arr[index] != <span class="number">0</span>) &#123;  </span><br><span class="line">       arr[<span class="number">0</span>] += arr[index];  </span><br><span class="line">       index++;  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">   <span class="keyword">int</span> *a;  </span><br><span class="line">   a = (<span class="keyword">int</span> *)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="keyword">int</span>) * <span class="number">12</span>);  </span><br><span class="line">   <span class="keyword">int</span> index = <span class="number">1</span>;  </span><br><span class="line">   <span class="keyword">for</span> (index = <span class="number">1</span>; index &lt;= <span class="number">10</span>; ++index) &#123;  </span><br><span class="line">       a[index] = index;  </span><br><span class="line">   &#125;  </span><br><span class="line">   kernel&lt;&lt;&lt;<span class="number">1</span>, <span class="number">1</span>&gt;&gt;&gt;(a);  </span><br><span class="line">   checkCudaErrors(cudaDeviceSynchronize());  </span><br><span class="line">   <span class="built_in">printf</span>(<span class="string">&quot;%d\n&quot;</span>, a[<span class="number">0</span>]);  </span><br><span class="line">   <span class="built_in">free</span>(a);  </span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这样就完成了我们的 debug 工作，此时输出结果为：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">0</span><br><span class="line">CUDA error at /home/chivier/Projects/cudatest/08-errhandle/hello.cu:23 code=700(cudaErrorIllegalAddress) &quot;cudaDeviceSynchronize()&quot;</span><br></pre></td></tr></table></figure>

<p>这里的 IllegalAddress 就说明了地址无法正常使用。</p>
<h3 id="4-2-5-cudaMemcpy"><a href="#4-2-5-cudaMemcpy" class="headerlink" title="4.2.5 cudaMemcpy"></a>4.2.5 cudaMemcpy</h3><p>那么正确的做法是什么呢？我们如果需要通信两个地方的内存，需要借助 cudaMemcpy 函数。</p>
<p>下面是正确的改正方法：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">  </span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">kernel</span><span class="params">(<span class="keyword">int</span> *arr)</span> </span>&#123;  </span><br><span class="line">   arr[<span class="number">0</span>] = <span class="number">0</span>;  </span><br><span class="line">   <span class="keyword">int</span> index = <span class="number">1</span>;  </span><br><span class="line">   <span class="keyword">while</span> (arr[index] != <span class="number">0</span>) &#123;  </span><br><span class="line">       arr[<span class="number">0</span>] += arr[index];  </span><br><span class="line">       index++;  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">   <span class="keyword">int</span> *a;  </span><br><span class="line">   a = (<span class="keyword">int</span> *)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="keyword">int</span>) * <span class="number">12</span>);  </span><br><span class="line">   <span class="keyword">int</span> index = <span class="number">1</span>;  </span><br><span class="line">   <span class="keyword">for</span> (index = <span class="number">1</span>; index &lt;= <span class="number">10</span>; ++index) &#123;  </span><br><span class="line">       a[index] = index;  </span><br><span class="line">   &#125;  </span><br><span class="line">  </span><br><span class="line">   <span class="keyword">int</span> *cuda_a;  </span><br><span class="line">   cudaMalloc(&amp;cuda_a, <span class="keyword">sizeof</span>(<span class="keyword">int</span>) * <span class="number">12</span>);  </span><br><span class="line">   cudaMemcpy(cuda_a, a, <span class="keyword">sizeof</span>(<span class="keyword">int</span>) * <span class="number">12</span>, cudaMemcpyHostToDevice);  </span><br><span class="line">   kernel&lt;&lt;&lt;<span class="number">1</span>, <span class="number">1</span>&gt;&gt;&gt;(cuda_a);  </span><br><span class="line">   cudaMemcpy(a, cuda_a, <span class="keyword">sizeof</span>(<span class="keyword">int</span>) * <span class="number">12</span>, cudaMemcpyDeviceToHost);  </span><br><span class="line">  </span><br><span class="line">   checkCudaErrors(cudaDeviceSynchronize());  </span><br><span class="line">   <span class="built_in">printf</span>(<span class="string">&quot;%d\n&quot;</span>, a[<span class="number">0</span>]);  </span><br><span class="line">   <span class="built_in">free</span>(a);  </span><br><span class="line">   cudaFree(cuda_a);  </span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>结果是：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">55</span><br></pre></td></tr></table></figure>

<p>这个时候我们可以总结一般的 cuda 变成行为方法：</p>
<ol>
<li>Generate Data/Read Data</li>
<li>Copy: Host-&gt;Device</li>
<li>Calculate</li>
<li>Copy: Device-&gt;Host</li>
<li>Print out</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这里说明一下：</span><br><span class="line">cudaMemcpy可以自动实现同步工作，可以省去cudaDeviceSynchronize。</span><br></pre></td></tr></table></figure>

<h3 id="4-2-4-Unified-Memory"><a href="#4-2-4-Unified-Memory" class="headerlink" title="4.2.4 Unified Memory"></a>4.2.4 Unified Memory</h3><p>首先先给出官方的一份博客：<a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/unified-memory-in-cuda-6/">Unified Memory</a></p>
<p>我们把上面的例子直接改写：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">  </span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">kernel</span><span class="params">(<span class="keyword">int</span> *arr)</span> </span>&#123;  </span><br><span class="line">   arr[<span class="number">0</span>] = <span class="number">0</span>;  </span><br><span class="line">   <span class="keyword">int</span> index = <span class="number">1</span>;  </span><br><span class="line">   <span class="keyword">while</span> (arr[index] != <span class="number">0</span>) &#123;  </span><br><span class="line">       arr[<span class="number">0</span>] += arr[index];  </span><br><span class="line">       index++;  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">   <span class="keyword">int</span> *a;  </span><br><span class="line">   checkCudaErrors(cudaMallocManaged(&amp;a, <span class="keyword">sizeof</span>(<span class="keyword">int</span>) * <span class="number">12</span>));  </span><br><span class="line">   <span class="keyword">int</span> index = <span class="number">1</span>;  </span><br><span class="line">   <span class="keyword">for</span> (index = <span class="number">1</span>; index &lt;= <span class="number">10</span>; ++index) &#123;  </span><br><span class="line">       a[index] = index;  </span><br><span class="line">   &#125;  </span><br><span class="line">  </span><br><span class="line">   kernel&lt;&lt;&lt;<span class="number">1</span>, <span class="number">1</span>&gt;&gt;&gt;(a);  </span><br><span class="line">      </span><br><span class="line">   checkCudaErrors(cudaDeviceSynchronize());  </span><br><span class="line">   <span class="built_in">printf</span>(<span class="string">&quot;%d\n&quot;</span>, a[<span class="number">0</span>]);  </span><br><span class="line">   cudaFree(a);  </span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Unified Memory 申请的空间在 Host 和 Device 上都直接可以正常使用。节省代码，但是代价也是随之而来的，我们需要使用一定的性能作为代价。这个有时是劣势，但有些时候这种方法反而可以达到优化的效果。</p>
<h2 id="4-3-变量传输和计算"><a href="#4-3-变量传输和计算" class="headerlink" title="4.3 变量传输和计算"></a>4.3 变量传输和计算</h2><p>这里我们简单介绍一下一般的循环模式。</p>
<h3 id="4-3-1-grid-stride-loop"><a href="#4-3-1-grid-stride-loop" class="headerlink" title="4.3.1 grid-stride loop"></a>4.3.1 grid-stride loop</h3><p>在 cuda 中，并行的模式有一点特殊，我们称为 grid-stride loop。</p>
<p>简单解释一下：</p>
<p>如果我们希望对一个循环进行并行，直观的想法就是分配足够多的线程执行他。如果循环次数少于 1024, 我们完全可以使用一个 block, 里面分配 <u>循环次数对应的线程数</u> 从而达到目的。但是这里不是我们希望看到的结果，因为我们不能保证我们的循环次数一定小于 1024。我们用下面的例子说明我们是如何用 cuda 执行循环的：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">  </span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">kernel</span><span class="params">(<span class="keyword">int</span> *arr, <span class="keyword">int</span> n)</span> </span>&#123;  </span><br><span class="line">   <span class="keyword">for</span> (<span class="keyword">int</span> i = threadIdx.x; i &lt; n; i += blockDim.x) &#123;  </span><br><span class="line">       arr[i] = i;  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">   <span class="keyword">int</span> n = <span class="number">114</span>;  </span><br><span class="line">   <span class="keyword">int</span> *arr;  </span><br><span class="line">   checkCudaErrors(cudaMallocManaged(&amp;arr, n * <span class="keyword">sizeof</span>(<span class="keyword">int</span>)));  </span><br><span class="line">  </span><br><span class="line">   kernel&lt;&lt;&lt;<span class="number">1</span>, <span class="number">4</span>&gt;&gt;&gt;(arr, n);  </span><br><span class="line">  </span><br><span class="line">   checkCudaErrors(cudaDeviceSynchronize());  </span><br><span class="line">   <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;  </span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">&quot;arr[%d]: %d\n&quot;</span>, i, arr[i]);  </span><br><span class="line">   &#125;  </span><br><span class="line">  </span><br><span class="line">   cudaFree(arr);</span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>由于 cuda 的大显存设计，我们大可以将一个数组很大一块连续块装入，而且 cuda 的计算机制保证了 blockDim 不会太大（1024）。所以这里尽管是跳步加，也不会过多的伤害到空间局部性。在不同的设备上，我们可以通过直接调整 blockDim 进行性能的测试，从而调优。Debug 的时候也可以直接把 gridDim 改成 1 从而进行便捷的调试。此外，这种写法的逻辑非常好整理，也方便进行记忆。为此这种方法被人们广泛使用。</p>
<p>如果我们需要启用多个 Block, 那么我们面临一个新的问题，如果循环次数不是 blockDim 的整数倍我们如何处理？<br>这里的办法是非常国际通用的，大家广泛的使用如下写法：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> loopCount = .....;</span><br><span class="line"></span><br><span class="line">....</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> block_dim = ...;</span><br><span class="line"><span class="keyword">int</span> grid_dim = (loopCount - <span class="number">1</span>) / block_dim + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">call_kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;();</span><br></pre></td></tr></table></figure>

<p>好文推荐（示意图很好）： <a target="_blank" rel="noopener" href="http://alexminnaar.com/2019/08/02/grid-stride-loops.html">http://alexminnaar.com/2019/08/02/grid-stride-loops.html</a></p>
<p>官方博客中还推荐了另外一种非常灵活的写法：<br><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/cuda-pro-tip-write-flexible-kernels-grid-stride-loops/">https://developer.nvidia.com/blog/cuda-pro-tip-write-flexible-kernels-grid-stride-loops/</a></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">saxpy</span><span class="params">(<span class="keyword">int</span> n, <span class="keyword">float</span> a, <span class="keyword">float</span> *x, <span class="keyword">float</span> *y)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = blockIdx.x * blockDim.x + threadIdx.x; i &lt; n; i += blockDim.x * gridDim.x) &#123;</span><br><span class="line">		y[i] = a * x[i] + y[i];</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>相当与我们的 Flatten Mode 的编程方法，确保我们不会落下数据没有循环。</p>
<p>虽说大部分 C++17 的代码 cuda 可以跑起来，但是 STL 容器 cuda 并没有很好的适配和实现，如果需要用容器，我们需要自己定义 allocator。这里推荐一个 gist： <a target="_blank" rel="noopener" href="https://gist.github.com/CommitThis/1666517de32893e5dc4c441269f1029a">https://gist.github.com/CommitThis/1666517de32893e5dc4c441269f1029a</a></p>
<p>其中定义了：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">unified_alloc</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">using</span> value_type = T;</span><br><span class="line">    <span class="keyword">using</span> pointer = value_type*;</span><br><span class="line">    <span class="keyword">using</span> size_type = <span class="built_in">std</span>::<span class="keyword">size_t</span>;</span><br><span class="line"></span><br><span class="line">    unified_alloc() <span class="keyword">noexcept</span> = <span class="keyword">default</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">template</span> &lt;<span class="keyword">typename</span> U&gt;</span><br><span class="line">    unified_alloc(unified_alloc&lt;U&gt; <span class="keyword">const</span>&amp;) <span class="keyword">noexcept</span> &#123;&#125;</span><br><span class="line"></span><br><span class="line">    auto allocate(size_type n, const void* = 0) -&gt; value_type* &#123;</span><br><span class="line">        value_type * tmp;</span><br><span class="line">        <span class="keyword">auto</span> error = cudaMallocManaged((<span class="keyword">void</span>**)&amp;tmp, n * <span class="keyword">sizeof</span>(T));</span><br><span class="line">        <span class="keyword">if</span> (error != cudaSuccess) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="built_in">std</span>::runtime_error &#123; cudaGetErrorString(error) &#125;;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> tmp;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    auto deallocate(pointer p, size_type n) -&gt; void &#123;</span><br><span class="line">        <span class="keyword">if</span> (p) &#123;</span><br><span class="line">            <span class="keyword">auto</span> error = cudaFree(p);</span><br><span class="line">            <span class="keyword">if</span> (error != cudaSuccess) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="built_in">std</span>::runtime_error &#123; cudaGetErrorString(error) &#125;;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>如果此时我们需要定义 vector, 只需要：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">int</span>, unified_alloc&lt;<span class="keyword">int</span>&gt;&gt; arr(length);</span><br></pre></td></tr></table></figure>

<p>即可。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">这里vector即使定义成功了，我们使用的时候还是会有一些问题，因为__global__函数的参数限制问题，我们只能传递 arr.data() 从而获取类似数组地址的方法传递参数。综上所属，楞写 STL 不是一个好办法，下面的 thrust 中会介绍一些靠谱的办法。</span><br></pre></td></tr></table></figure>

<h3 id="4-3-2-算子传输"><a href="#4-3-2-算子传输" class="headerlink" title="4.3.2 算子传输"></a>4.3.2 算子传输</h3><p>在 cuda 编程的时候，经常遇到下面这个问题：</p>
<p>对于很多类似的函数，我们只需要更换其中的一个小函数即可，例如计算</p>
<p>$$<br>\sum_{i=0}^n Sin(a_i),;\sum_{i=0}^n Cos(a_i),;\sum_{i=0}^n Tan(a_i),;…<br>$$</p>
<p>很多时候只需要换一个算子，我们不需要重复构造很多个函数（减少函数数量其实是有一定好处的，对于模式类似的函数，如果适当利用 lambda 算子，我们可以减少编译器的编译负担）</p>
<p>这种时候我们可以采用两种方法。</p>
<p>第一种就是 lambda 算子：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">  </span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">Func</span>&gt;</span>  </span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">kernel</span><span class="params">(<span class="keyword">int</span> n, Func func)</span> </span>&#123;  </span><br><span class="line">   <span class="keyword">for</span> (<span class="keyword">int</span> i = blockDim.x * blockIdx.x + threadIdx.x;  </span><br><span class="line">        i &lt; n; i += blockDim.x * gridDim.x) &#123;  </span><br><span class="line">       func(i);  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">   <span class="keyword">int</span> n = <span class="number">10</span>;  </span><br><span class="line">   <span class="keyword">int</span> *arr;  </span><br><span class="line">  </span><br><span class="line">   cudaMallocManaged(&amp;arr, n * <span class="keyword">sizeof</span>(<span class="keyword">int</span>));  </span><br><span class="line">  </span><br><span class="line">   <span class="keyword">int</span> block_dim = <span class="number">128</span>;  </span><br><span class="line">   <span class="keyword">int</span> grid_dim = (n - <span class="number">1</span>) / block_dim + <span class="number">1</span>;  </span><br><span class="line">   kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(n, [=] __device__ (<span class="keyword">int</span> i) &#123;  </span><br><span class="line">       arr[i] = i;  </span><br><span class="line">   &#125;);  </span><br><span class="line">      </span><br><span class="line">   checkCudaErrors(cudaDeviceSynchronize());  </span><br><span class="line">   kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(n, [=] __device__ (<span class="keyword">int</span> i) &#123;  </span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">&quot;%d, %f\n&quot;</span>, i, sinf(arr[i]));  </span><br><span class="line">   &#125;);  </span><br><span class="line">  </span><br><span class="line">   checkCudaErrors(cudaDeviceSynchronize());  </span><br><span class="line">  </span><br><span class="line">   <span class="comment">// Compare  </span></span><br><span class="line">   <span class="comment">// for(int index = 0; index &lt; n; ++index) &#123;  </span></span><br><span class="line">   <span class="comment">//    printf(&quot;%d, %f\n&quot;, index, sinf(index));  </span></span><br><span class="line">   <span class="comment">//&#125;  </span></span><br><span class="line">  </span><br><span class="line">   cudaFree(arr);  </span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>注意 lambda 算子也是要在 GPU 上执行的，所以需要加上 _<em>device_</em> 进行修饰。</p>
<p>第二种方法就是使用类似于函数指针的方式。不过 cuda 中函数指针的定义非常严格：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">  </span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">Func</span>&gt;</span>  </span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">kernel</span><span class="params">(<span class="keyword">int</span> *arr, <span class="keyword">int</span> n, Func func)</span> </span>&#123;  </span><br><span class="line">   <span class="keyword">for</span> (<span class="keyword">int</span> i = blockDim.x * blockIdx.x + threadIdx.x;  </span><br><span class="line">        i &lt; n; i += blockDim.x * gridDim.x) &#123;  </span><br><span class="line">       func(arr, i);  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">funcop1</span> &#123;</span>  </span><br><span class="line">   <span class="function">__device__ <span class="keyword">void</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="keyword">int</span> *arr, <span class="keyword">int</span> i)</span> </span>&#123;  </span><br><span class="line">       arr[i] = i;  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;;  </span><br><span class="line">  </span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">funcop2</span> &#123;</span>  </span><br><span class="line">   <span class="function">__device__ <span class="keyword">void</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="keyword">int</span> *arr, <span class="keyword">int</span> i)</span> </span>&#123;  </span><br><span class="line">       <span class="built_in">printf</span>(<span class="string">&quot;%d %f\n&quot;</span>, arr[i], sinf(arr[i]));  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">   <span class="keyword">int</span> n = <span class="number">10</span>;  </span><br><span class="line">   <span class="keyword">int</span> *arr;  </span><br><span class="line">  </span><br><span class="line">   cudaMallocManaged(&amp;arr, n * <span class="keyword">sizeof</span>(<span class="keyword">int</span>));  </span><br><span class="line">  </span><br><span class="line">   <span class="keyword">int</span> block_dim = <span class="number">128</span>;  </span><br><span class="line">   <span class="keyword">int</span> grid_dim = (n - <span class="number">1</span>) / block_dim + <span class="number">1</span>;  </span><br><span class="line">   kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(arr, n, funcop1&#123;&#125;);  </span><br><span class="line">      </span><br><span class="line">   checkCudaErrors(cudaDeviceSynchronize());  </span><br><span class="line">   kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(arr, n, funcop2&#123;&#125;);  </span><br><span class="line">  </span><br><span class="line">   checkCudaErrors(cudaDeviceSynchronize());  </span><br><span class="line">  </span><br><span class="line">   <span class="comment">// Compare  </span></span><br><span class="line">   <span class="comment">// for(int index = 0; index &lt; n; ++index) &#123;  </span></span><br><span class="line">   <span class="comment">//    printf(&quot;%d, %f\n&quot;, index, sinf(index));  </span></span><br><span class="line">   <span class="comment">//&#125;  </span></span><br><span class="line">  </span><br><span class="line">   cudaFree(arr);  </span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>需要注意的是：</p>
<ol>
<li>由于地址类型在传递时，CPU 和 GPU 不一致，所以需要用结构体封装一层</li>
<li>结构体中的函数是在 GPU 上计算，需要用 device 修饰</li>
<li>结构体中函数需要用 operator () 修饰</li>
</ol>
<p>到这里，基础部分已经基本结束。</p>
<h2 id="4-4-高级计算行为"><a href="#4-4-高级计算行为" class="headerlink" title="4.4 高级计算行为"></a>4.4 高级计算行为</h2><h3 id="4-4-1-thrust"><a href="#4-4-1-thrust" class="headerlink" title="4.4.1 thrust"></a>4.4.1 thrust</h3><p>之前我们提及过，在 cuda 中无法使用 C++ STL 进行编程，为此我们需要使用 Nviida 提供的黑科技： thrust。</p>
<p>首先简单介绍一下，thrust 库被称为： Template library for CUDA，自从 cuda 4.0 就开始有了（甚至比 unified Memory 还早是我没想到的）。主要目的是对标 C++ STL。简化 HPC 编程。</p>
<h4 id="4-4-1-1-vector-对标"><a href="#4-4-1-1-vector-对标" class="headerlink" title="4.4.1.1 vector 对标"></a>4.4.1.1 vector 对标</h4><p>C++ STL 中 vector 容器属实是一个使用主力了，在 thrust 中我们也有对应的实现，分别为：<code>universal_vector</code>，<code>host_vector</code>，<code>device_vector</code>。他们的用途根据名字应该已经可以猜到了。使用 thrust 之后，我们可以直接使用 <code>=</code> 进行数值拷贝。Cuda 中已经完成了这一部分的重载工作。</p>
<p>下面列举一个例子：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">  </span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">Func</span>&gt;</span>  </span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">kernel</span><span class="params">(<span class="keyword">int</span> n, Func func)</span> </span>&#123;  </span><br><span class="line">   <span class="keyword">for</span> (<span class="keyword">int</span> i = blockDim.x * blockIdx.x + threadIdx.x;  </span><br><span class="line">        i &lt; n; i += blockDim.x * gridDim.x) &#123;  </span><br><span class="line">       func(i);  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">   <span class="keyword">int</span> n = <span class="number">65536</span>;  </span><br><span class="line">  </span><br><span class="line">   <span class="keyword">int</span> block_dim = <span class="number">128</span>;  </span><br><span class="line">   <span class="keyword">int</span> grid_dim = (n - <span class="number">1</span>) / block_dim;  </span><br><span class="line">      </span><br><span class="line">   <span class="function">thrust::host_vector&lt;<span class="keyword">float</span>&gt; <span class="title">x_host</span><span class="params">(n)</span></span>;  </span><br><span class="line">   <span class="function">thrust::host_vector&lt;<span class="keyword">float</span>&gt; <span class="title">y_host</span><span class="params">(n)</span></span>;  </span><br><span class="line">  </span><br><span class="line">   thrust::generate(x_host.begin(), x_host.end(), []&#123;<span class="keyword">return</span> <span class="built_in">std</span>::rand() / <span class="number">3.0</span>;&#125;);  </span><br><span class="line">   thrust::generate(y_host.begin(), y_host.end(), []&#123;<span class="keyword">return</span> <span class="built_in">std</span>::rand() / <span class="number">11.0</span>;&#125;);  </span><br><span class="line">  </span><br><span class="line">   <span class="built_in">printf</span>(<span class="string">&quot;%f + %f = \n&quot;</span>, x_host[<span class="number">0</span>], y_host[<span class="number">0</span>]);  </span><br><span class="line">  </span><br><span class="line">   <span class="function">thrust::device_vector&lt;<span class="keyword">float</span>&gt; <span class="title">x_dev</span><span class="params">(n)</span></span>;  </span><br><span class="line">   <span class="function">thrust::device_vector&lt;<span class="keyword">float</span>&gt; <span class="title">y_dev</span><span class="params">(n)</span></span>;  </span><br><span class="line">   x_dev = x_host;  </span><br><span class="line">   y_dev = y_host;  </span><br><span class="line">  </span><br><span class="line">   kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(n, [x = x_dev.data(), y = y_dev.data()] __device__ (<span class="keyword">int</span> index)&#123;  </span><br><span class="line">       x[index] = x[index] + y[index];  </span><br><span class="line">   &#125;);  </span><br><span class="line">  </span><br><span class="line">   checkCudaErrors(cudaDeviceSynchronize());  </span><br><span class="line">   x_host = x_dev;  </span><br><span class="line">  </span><br><span class="line">   <span class="built_in">printf</span>(<span class="string">&quot;%f\n&quot;</span>, x_host[<span class="number">0</span>]);  </span><br><span class="line">  </span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">601429824.000000 + 151421216.000000 =   </span><br><span class="line">752851072.000000</span><br></pre></td></tr></table></figure>

<p>本程序成功的展示了 thrust_vector 之间互相 copy 的行为，这里辅助使用了一下 thrust :: generate 用于生成随机序列。（这就是前一节花篇幅辅助介绍算子传递的原因，lambda 算子在 thrust 中会有广泛的应用，尽管这是 C++20 才应该有的特性，但是很多编译器早早的提供了支持「赞美 LLVM」。）</p>
<h4 id="4-4-1-2-other-thrusts"><a href="#4-4-1-2-other-thrusts" class="headerlink" title="4.4.1.2 other thrusts"></a>4.4.1.2 other thrusts</h4><p>Thrust 中还有很多和 STL 对标的内容，例如 For_each、sort、count_if 这些都是非常常用的 thrust 函数，使用方法和 generate 几乎一致，没有过多差别。</p>
<p><a target="_blank" rel="noopener" href="https://thrust.github.io/doc/namespacethrust.html">https://thrust.github.io/doc/namespacethrust.html</a></p>
<p>这里个人推荐在上面的网页中查找 thrust 的 API。Doxygen 提供了详细的文档管理。我们可以查到具体的使用方法和相对应的例子。</p>
<h3 id="4-4-2-atomic"><a href="#4-4-2-atomic" class="headerlink" title="4.4.2 atomic"></a>4.4.2 atomic</h3><p>说道这个问题，很容易会和生产-消费的问题联系到一起。这里先列举一个翻车的例子：</p>
<p>(下面这个例子中顺便引入了一个 trick, 在 cuda 中我们是可以使用全局变量的，但是和我们管理内存的方式是一样的，一个全局变量也是有“位置”的，我们在 GPU 和 CPU 上的全局变量需要使用 cudaMemcpyFromSymbol 进行拷贝)</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">__device__ <span class="keyword">float</span> sum = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">Func</span>&gt;</span></span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">kernel</span><span class="params">(<span class="keyword">int</span> n, Func func)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = blockDim.x * blockIdx.x + threadIdx.x;</span><br><span class="line">         i &lt; n; i += blockDim.x * gridDim.x) &#123;</span><br><span class="line">        func(i);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n = <span class="number">65536</span>;</span><br><span class="line">    <span class="keyword">int</span> *arr;</span><br><span class="line">    <span class="keyword">float</span> result = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    cudaMallocManaged(&amp;arr, n * <span class="keyword">sizeof</span>(<span class="keyword">int</span>));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> block_dim = <span class="number">128</span>;</span><br><span class="line">    <span class="keyword">int</span> grid_dim = (n - <span class="number">1</span>) / block_dim;</span><br><span class="line">    kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(n, [=] __device__ (<span class="keyword">int</span> i) &#123;</span><br><span class="line">        arr[i] = i;</span><br><span class="line">    &#125;);</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(n, [=] __device__ (<span class="keyword">int</span> i) &#123;</span><br><span class="line">        sum += sinf(arr[i]);</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    cudaMemcpyFromSymbol(&amp;result, sum, <span class="keyword">sizeof</span>(<span class="keyword">float</span>), <span class="number">0</span>, cudaMemcpyDeviceToHost);</span><br><span class="line">    checkCudaErrors(cudaDeviceSynchronize());</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%f\n&quot;</span>, result);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Compare</span></span><br><span class="line">    result = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> index = <span class="number">0</span>; index &lt; n; ++index) &#123;</span><br><span class="line">        result += sinf(index);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%f&quot;</span>, result);</span><br><span class="line"></span><br><span class="line">    cudaFree(arr);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上面的例子非常好理解，是我们之前一个 lambda 例子的微微改进，我们之所以说明这个例子是因为这个程序行为非常“鬼畜”。每次执行结果都可能不同。那么问题在哪里呢？</p>
<p>在于</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sum += sinf (arr[i])</span><br></pre></td></tr></table></figure>
<p>操作实际上不是原子的。我们的不同的 i 会抢占对于 sum 的使用。</p>
<p>修正方法也很简单：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">  </span><br><span class="line">__device__ <span class="keyword">float</span> sum = <span class="number">0</span>;  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">Func</span>&gt;</span>  </span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">kernel</span><span class="params">(<span class="keyword">int</span> n, Func func)</span> </span>&#123;  </span><br><span class="line">   <span class="keyword">for</span> (<span class="keyword">int</span> i = blockDim.x * blockIdx.x + threadIdx.x;  </span><br><span class="line">        i &lt; n; i += blockDim.x * gridDim.x) &#123;  </span><br><span class="line">       func(i);  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">   <span class="keyword">int</span> n = <span class="number">65536</span>;  </span><br><span class="line">   <span class="keyword">int</span> *arr;  </span><br><span class="line">   <span class="keyword">float</span> result = <span class="number">0</span>;  </span><br><span class="line">  </span><br><span class="line">   cudaMallocManaged(&amp;arr, n * <span class="keyword">sizeof</span>(<span class="keyword">int</span>));  </span><br><span class="line">  </span><br><span class="line">   <span class="keyword">int</span> block_dim = <span class="number">128</span>;  </span><br><span class="line">   <span class="keyword">int</span> grid_dim = (n - <span class="number">1</span>) / block_dim;  </span><br><span class="line">   kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(n, [=] __device__ (<span class="keyword">int</span> i) &#123;  </span><br><span class="line">       arr[i] = i;  </span><br><span class="line">   &#125;);  </span><br><span class="line">      </span><br><span class="line">      </span><br><span class="line">   kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(n, [=] __device__ (<span class="keyword">int</span> i) &#123;  </span><br><span class="line">       atomicAdd(&amp;sum, sinf(arr[i]));  </span><br><span class="line">   &#125;);  </span><br><span class="line">  </span><br><span class="line">   cudaMemcpyFromSymbol(&amp;result, sum, <span class="keyword">sizeof</span>(<span class="keyword">float</span>), <span class="number">0</span>, cudaMemcpyDeviceToHost);  </span><br><span class="line">   checkCudaErrors(cudaDeviceSynchronize());  </span><br><span class="line">      </span><br><span class="line">   <span class="built_in">printf</span>(<span class="string">&quot;%f\n&quot;</span>, result);  </span><br><span class="line">  </span><br><span class="line">   <span class="comment">// Compare  </span></span><br><span class="line">   result = <span class="number">0</span>;  </span><br><span class="line">   <span class="keyword">for</span>(<span class="keyword">int</span> index = <span class="number">0</span>; index &lt; n; ++index) &#123;  </span><br><span class="line">       result += sinf(index);  </span><br><span class="line">   &#125;  </span><br><span class="line">   <span class="built_in">printf</span>(<span class="string">&quot;%f\n&quot;</span>, result);  </span><br><span class="line">  </span><br><span class="line">   cudaFree(arr);  </span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>即改为：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">atomicAdd(&amp;sum, sinf(arr[i]));  </span><br></pre></td></tr></table></figure>

<p>此时就只有一点点由于加法顺序不同导致的精度误差了。这一些误差是可以被接受的。</p>
<p>那么我们继续分析一下这个问题，在 cuda 内部我们是如何实现原子的。这个问题在“编译原理”课程中可能有过类似的解答。<br>为了实现一个操作的原子性，我们需要“盯着他的数值”，只有我们“计算完成”“原始值也没变化”的时候，我们才能成功的优化。</p>
<p>换而言之，类似 atomicAdd, 我们又如下的步骤：</p>
<ul>
<li>Value_prev = Target</li>
<li>Target += …</li>
<li>Return Value_prev</li>
</ul>
<p>如果此时 Value_prev 和现在的状态一致的，就可以进行更新。</p>
<p>Cuda 中还有这些原子函数：</p>
<p>atomicAdd (dst, src)<br>atomicSub(dst, src)<br>atomicOr(dst, src)<br>atomicAnd(dst, src)<br>atomicXor(dst, src)<br>atomicMax(dst, src)<br>atomicMin(dst, src)</p>
<p>他们都有返回值，返回违背更改前的数值。</p>
<p>更加一般的，我们可以自己定义一个属于自己的原子操作：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">  </span><br><span class="line">__device__ <span class="keyword">float</span> sum = <span class="number">0</span>;  </span><br><span class="line">  </span><br><span class="line"><span class="function">__device__ <span class="keyword">float</span> <span class="title">my_atom_add</span><span class="params">(<span class="keyword">float</span> *dst, <span class="keyword">float</span> src)</span></span>&#123;  </span><br><span class="line">   <span class="keyword">int</span> old = __float_as_int(*dst);  </span><br><span class="line">   <span class="keyword">int</span> expect;  </span><br><span class="line">   <span class="keyword">do</span> &#123;  </span><br><span class="line">       expect = old;  </span><br><span class="line">       old = atomicCAS((<span class="keyword">int</span> *)dst, expect,  </span><br><span class="line">               __float_as_int(__int_as_float(expect) + sinf(src)));  </span><br><span class="line">   &#125; <span class="keyword">while</span>(expect != old);  </span><br><span class="line">   <span class="keyword">return</span> old;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">template</span> &lt;<span class="class"><span class="keyword">class</span> <span class="title">Func</span>&gt;</span>  </span><br><span class="line"><span class="function">__global__ <span class="keyword">void</span> <span class="title">kernel</span><span class="params">(<span class="keyword">int</span> n, Func func)</span> </span>&#123;  </span><br><span class="line">   <span class="keyword">for</span> (<span class="keyword">int</span> i = blockDim.x * blockIdx.x + threadIdx.x;  </span><br><span class="line">        i &lt; n; i += blockDim.x * gridDim.x) &#123;  </span><br><span class="line">       func(i);  </span><br><span class="line">   &#125;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;  </span><br><span class="line">   <span class="keyword">int</span> n = <span class="number">65536</span>;  </span><br><span class="line">   <span class="keyword">int</span> *arr;  </span><br><span class="line">   <span class="keyword">float</span> result = <span class="number">0</span>;  </span><br><span class="line">  </span><br><span class="line">   cudaMallocManaged(&amp;arr, n * <span class="keyword">sizeof</span>(<span class="keyword">int</span>));  </span><br><span class="line">  </span><br><span class="line">   <span class="keyword">int</span> block_dim = <span class="number">128</span>;  </span><br><span class="line">   <span class="keyword">int</span> grid_dim = (n - <span class="number">1</span>) / block_dim;  </span><br><span class="line">   kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(n, [=] __device__ (<span class="keyword">int</span> i) &#123;  </span><br><span class="line">       arr[i] = i;  </span><br><span class="line">   &#125;);  </span><br><span class="line">      </span><br><span class="line">      </span><br><span class="line">   kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(n, [=] __device__ (<span class="keyword">int</span> i) &#123;  </span><br><span class="line">       my_atom_add(&amp;sum, arr[i]);  </span><br><span class="line">   &#125;);  </span><br><span class="line">  </span><br><span class="line">   cudaMemcpyFromSymbol(&amp;result, sum, <span class="keyword">sizeof</span>(<span class="keyword">float</span>), <span class="number">0</span>, cudaMemcpyDeviceToHost);  </span><br><span class="line">   checkCudaErrors(cudaDeviceSynchronize());  </span><br><span class="line">      </span><br><span class="line">   <span class="built_in">printf</span>(<span class="string">&quot;%f\n&quot;</span>, result);  </span><br><span class="line">  </span><br><span class="line">   <span class="comment">// Compare  </span></span><br><span class="line">   result = <span class="number">0</span>;  </span><br><span class="line">   <span class="keyword">for</span>(<span class="keyword">int</span> index = <span class="number">0</span>; index &lt; n; ++index) &#123;  </span><br><span class="line">       result += sinf(index);  </span><br><span class="line">   &#125;  </span><br><span class="line">   <span class="built_in">printf</span>(<span class="string">&quot;%f\n&quot;</span>, result);  </span><br><span class="line">  </span><br><span class="line">   cudaFree(arr);  </span><br><span class="line">   <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>我们使用 atomoicCAS 按照类似的逻辑步骤：</p>
<ul>
<li>记录维护原始值</li>
<li>试图 CAS 更改</li>
<li>成功改动之后可以停下</li>
</ul>
<p>这里使用了一些技巧，结合下面的官网借口解释：</p>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#atomiccas">https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#atomiccas</a></p>
<img src="image5.png" width="80%" height="80%">

<p>AtomicCAS 是一个只支持整数的借口，如果需要浮点支持，需要进行 as float 进行转换。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">这里不难看出另一个问题，如果我们的原子操作被严格执行，那么原子操作会成为一个严重的瓶颈。相当于所有的数据都要过一遍着一个原子操作。但是实际上GPU跑起来还是非常快的，这是因为GPU根据blockDim和gridDim进行了操作，部分串行。以求和为例，我们会将数据分成几个大块，分别计算部分和，最后再进行规约。利用这种方法保证了此处不会成为瓶颈。</span><br><span class="line">因此也不难看出：原子操作都是可以 Fork-And-Merge 的操作。</span><br></pre></td></tr></table></figure>


      
    </div>
    <footer class="article-footer">
      
        <a data-url="https://chivier.github.io/2022/02/20/2022/2202-CudaProgramming/" data-id="ckzux2hhi0000qcc19yuo5565" class="article-share-link" data-share="baidu" data-title="2202-CudaProgramming">Share</a>
      

      

      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Cuda/" rel="tag">Cuda</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Develop/" rel="tag">Develop</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Parallel/" rel="tag">Parallel</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2022/03/09/2022/2203-TFcustom/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          2203-TFcustom
        
      </div>
    </a>
  
  
    <a href="/2022/02/04/2022/2202-QemuTest/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">2202-QemuTest</div>
    </a>
  
</nav>

  
</article>

</section>
      
      <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Android/">Android</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Android/Android-tricks/">Android tricks</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Architechture/">Architechture</a><span class="category-list-count">5</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Architechture/Compiler/">Compiler</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Architechture/Compiler/FP/">FP</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Architechture/Dataflow/">Dataflow</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Architechture/Parallel/">Parallel</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Blog/">Blog</a><span class="category-list-count">2</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Blog/Hexo/">Hexo</a><span class="category-list-count">2</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/COD/">COD</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Compiler/">Compiler</a><span class="category-list-count">2</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Compiler/DSL/">DSL</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Compiler/Translator/">Translator</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Cuda/">Cuda</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Cuda/Parallel/">Parallel</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Cuda/Parallel/Develop/">Develop</a><span class="category-list-count">1</span></li></ul></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Develop/">Develop</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Develop/Graphics/">Graphics</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Docker/">Docker</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Docker/Tutorial/">Tutorial</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Electronic/">Electronic</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/ICS/">ICS</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/ICS/TA/">TA</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/LLVM/">LLVM</a><span class="category-list-count">2</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/LLVM/Compiler/">Compiler</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/LLVM/DSL/">DSL</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a><span class="category-list-count">20</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/Linux-Learning/">Linux Learning</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/Linux-Tricks/">Linux Tricks</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/Linux-tricks/">Linux tricks</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/Qemu/">Qemu</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/Qemu/Compiler/">Compiler</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/Tricks/">Tricks</a><span class="category-list-count">2</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Maths/">Maths</a><span class="category-list-count">3</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Maths/Logic/">Logic</a><span class="category-list-count">3</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Operating-System/">Operating System</a><span class="category-list-count">2</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Operating-System/Concepts/">Concepts</a><span class="category-list-count">2</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Others/">Others</a><span class="category-list-count">2</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Others/Latex/">Latex</a><span class="category-list-count">2</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Programming-language/">Programming language</a><span class="category-list-count">7</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Programming-language/Assembly/">Assembly</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Programming-language/C/">C</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Programming-language/Rust/">Rust</a><span class="category-list-count">4</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Rust/">Rust</a><span class="category-list-count">2</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Rust/Develop/">Develop</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Rust/Develop/DSL/">DSL</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Rust/Rust-Project/">Rust Project</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/System/">System</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tensorflow/">Tensorflow</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Tensorflow/Cuda/">Cuda</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Tensorflow/Cuda/Deeplearning/">Deeplearning</a><span class="category-list-count">1</span></li></ul></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tools/">Tools</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Windows/">Windows</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Work/">Work</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Work/UI/">UI</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/linux/">linux</a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/linux/ubuntu/">ubuntu</a><span class="category-list-count">1</span></li></ul></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Architechture/" rel="tag">Architechture</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Assembly/" rel="tag">Assembly</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Blog/" rel="tag">Blog</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/C/" rel="tag">C</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/COD/" rel="tag">COD</a><span class="tag-list-count">9</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Circuits/" rel="tag">Circuits</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Compiler/" rel="tag">Compiler</a><span class="tag-list-count">5</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Cuda/" rel="tag">Cuda</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/DSL/" rel="tag">DSL</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Dataflow/" rel="tag">Dataflow</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Deeplearning/" rel="tag">Deeplearning</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Develop/" rel="tag">Develop</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Docker/" rel="tag">Docker</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Electronic/" rel="tag">Electronic</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/FP/" rel="tag">FP</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Git/" rel="tag">Git</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Graphics/" rel="tag">Graphics</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Hexo/" rel="tag">Hexo</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ICS/" rel="tag">ICS</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/LLVM/" rel="tag">LLVM</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Latex/" rel="tag">Latex</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux/" rel="tag">Linux</a><span class="tag-list-count">24</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux-Software/" rel="tag">Linux Software</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux-Tricks/" rel="tag">Linux Tricks</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Logic/" rel="tag">Logic</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Maths/" rel="tag">Maths</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Network/" rel="tag">Network</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Parallel/" rel="tag">Parallel</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Programming-language/" rel="tag">Programming language</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Qemu/" rel="tag">Qemu</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Rust/" rel="tag">Rust</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Shadowsocks/" rel="tag">Shadowsocks</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Shell/" rel="tag">Shell</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Softwares/" rel="tag">Softwares</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/System/" rel="tag">System</a><span class="tag-list-count">14</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/TA/" rel="tag">TA</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tensorflow/" rel="tag">Tensorflow</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tools/" rel="tag">Tools</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Translator/" rel="tag">Translator</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tricks/" rel="tag">Tricks</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tutorial/" rel="tag">Tutorial</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/UI/" rel="tag">UI</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Ubuntu/" rel="tag">Ubuntu</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Verilog/" rel="tag">Verilog</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Web/" rel="tag">Web</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Windows/" rel="tag">Windows</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Work/" rel="tag">Work</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/" rel="tag">linux</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ubuntu/" rel="tag">ubuntu</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Architechture/" style="font-size: 14.29px;">Architechture</a> <a href="/tags/Assembly/" style="font-size: 11.43px;">Assembly</a> <a href="/tags/Blog/" style="font-size: 11.43px;">Blog</a> <a href="/tags/C/" style="font-size: 10px;">C</a> <a href="/tags/COD/" style="font-size: 17.14px;">COD</a> <a href="/tags/Circuits/" style="font-size: 10px;">Circuits</a> <a href="/tags/Compiler/" style="font-size: 14.29px;">Compiler</a> <a href="/tags/Cuda/" style="font-size: 11.43px;">Cuda</a> <a href="/tags/DSL/" style="font-size: 12.86px;">DSL</a> <a href="/tags/Dataflow/" style="font-size: 12.86px;">Dataflow</a> <a href="/tags/Deeplearning/" style="font-size: 10px;">Deeplearning</a> <a href="/tags/Develop/" style="font-size: 12.86px;">Develop</a> <a href="/tags/Docker/" style="font-size: 10px;">Docker</a> <a href="/tags/Electronic/" style="font-size: 10px;">Electronic</a> <a href="/tags/FP/" style="font-size: 10px;">FP</a> <a href="/tags/Git/" style="font-size: 10px;">Git</a> <a href="/tags/Graphics/" style="font-size: 10px;">Graphics</a> <a href="/tags/Hexo/" style="font-size: 11.43px;">Hexo</a> <a href="/tags/ICS/" style="font-size: 10px;">ICS</a> <a href="/tags/LLVM/" style="font-size: 11.43px;">LLVM</a> <a href="/tags/Latex/" style="font-size: 11.43px;">Latex</a> <a href="/tags/Linux/" style="font-size: 20px;">Linux</a> <a href="/tags/Linux-Software/" style="font-size: 10px;">Linux Software</a> <a href="/tags/Linux-Tricks/" style="font-size: 10px;">Linux Tricks</a> <a href="/tags/Logic/" style="font-size: 12.86px;">Logic</a> <a href="/tags/Maths/" style="font-size: 12.86px;">Maths</a> <a href="/tags/Network/" style="font-size: 11.43px;">Network</a> <a href="/tags/Parallel/" style="font-size: 11.43px;">Parallel</a> <a href="/tags/Programming-language/" style="font-size: 15.71px;">Programming language</a> <a href="/tags/Qemu/" style="font-size: 10px;">Qemu</a> <a href="/tags/Rust/" style="font-size: 15.71px;">Rust</a> <a href="/tags/Shadowsocks/" style="font-size: 10px;">Shadowsocks</a> <a href="/tags/Shell/" style="font-size: 10px;">Shell</a> <a href="/tags/Softwares/" style="font-size: 10px;">Softwares</a> <a href="/tags/System/" style="font-size: 18.57px;">System</a> <a href="/tags/TA/" style="font-size: 10px;">TA</a> <a href="/tags/Tensorflow/" style="font-size: 10px;">Tensorflow</a> <a href="/tags/Tools/" style="font-size: 10px;">Tools</a> <a href="/tags/Translator/" style="font-size: 10px;">Translator</a> <a href="/tags/Tricks/" style="font-size: 11.43px;">Tricks</a> <a href="/tags/Tutorial/" style="font-size: 10px;">Tutorial</a> <a href="/tags/UI/" style="font-size: 10px;">UI</a> <a href="/tags/Ubuntu/" style="font-size: 10px;">Ubuntu</a> <a href="/tags/Verilog/" style="font-size: 15.71px;">Verilog</a> <a href="/tags/Web/" style="font-size: 10px;">Web</a> <a href="/tags/Windows/" style="font-size: 11.43px;">Windows</a> <a href="/tags/Work/" style="font-size: 10px;">Work</a> <a href="/tags/linux/" style="font-size: 10px;">linux</a> <a href="/tags/ubuntu/" style="font-size: 10px;">ubuntu</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/03/">March 2022</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/02/">February 2022</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/01/">January 2022</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/11/">November 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/10/">October 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/03/">March 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">September 2020</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">December 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">November 2019</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">September 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/06/">June 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/04/">April 2019</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/02/">February 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/03/28/2022/2203-voronoi%E6%9C%B4%E7%B4%A0%E5%BC%80%E5%8F%91%E7%AC%94%E8%AE%B0/">2203-voronoi朴素开发笔记</a>
          </li>
        
          <li>
            <a href="/2022/03/28/2022/2203-%E5%B9%B6%E8%A1%8C%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84%E7%AC%94%E8%AE%B01/">2203-并行体系结构笔记1</a>
          </li>
        
          <li>
            <a href="/2022/03/23/2022/2203-%E6%B5%AE%E7%82%B9%E6%95%B0%E9%97%AE%E9%A2%98/">2203-浮点数问题</a>
          </li>
        
          <li>
            <a href="/2022/03/09/2022/2203-TFcustom/">2203-TFcustom</a>
          </li>
        
          <li>
            <a href="/2022/02/20/2022/2202-CudaProgramming/">2202-CudaProgramming</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Links</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="https://github.com/Chivier" target="_blank">主题作者</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
      
    </div>
    <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2022 Chivier Humber<br>
      Powered by <a href="//hexo.io/" target="_blank">Hexo</a>
      .
      Theme by <a href="https://github.com/xiangming/landscape-plus" target="_blank">Landscape-plus</a>
    </div>
  </div>
</footer>
  </div>
  <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/Whoami" class="mobile-nav-link">About</a>
  
    <a href="/categories" class="mobile-nav-link">Catagories</a>
  
</nav>
  <!-- totop start -->
<div id="totop">
<a title="totop"><img src="/img/scrollup.png"/></a>
</div>

<!-- totop end -->


<!-- 百度分享 start -->

<div id="article-share-box" class="article-share-box">
  <div id="bdshare" class="bdsharebuttonbox article-share-links">
    <a class="article-share-weibo" data-cmd="tsina" title="分享到新浪微博"></a>
    <a class="article-share-weixin" data-cmd="weixin" title="分享到微信"></a>
    <a class="article-share-qq" data-cmd="sqq" title="分享到QQ"></a>
    <a class="article-share-renren" data-cmd="renren" title="分享到人人网"></a>
    <a class="article-share-more" data-cmd="more" title="更多"></a>
  </div>
</div>
<script>
  function SetShareData(cmd, config) {
    if (shareDataTitle && shareDataUrl) {
      config.bdText = shareDataTitle;
      config.bdUrl = shareDataUrl;
    }
    return config;
  }
  window._bd_share_config={
    "common":{onBeforeClick: SetShareData},
    "share":{"bdCustomStyle":"/css/bdshare.css"}
  };
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='//bdimg.share.baidu.com/static/api/js/share.js?cdnversion='+~(-new Date()/36e5)];
</script>

<!-- 百度分享 end -->

<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<! -- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
                processEscapes: true
                    
}
  
        });
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
                  
}
    
        });
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Queue(function() {
            var all = MathJax.Hub.getAllJax(), i;
            for(i=0; i < all.length; i += 1) {
                            all[i].SourceElement().parentNode.className += ' has-jax';
                                    
            }
                
        });
</script>

<script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.5.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



<script src="/js/script.js"></script>


</div>
</body>
</html>
