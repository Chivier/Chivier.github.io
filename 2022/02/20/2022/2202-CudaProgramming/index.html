

<!DOCTYPE html>
<html lang="null" data-default-color-scheme=auto>

<meta name="google-site-verification" content="ZNN3oejaVPan2TBRaPKWT4_wyI6Q7MTk8LMpKv-4VYg" />


<head>
  <meta name="google-site-verification" content="ZNN3oejaVPan2TBRaPKWT4_wyI6Q7MTk8LMpKv-4VYg" />
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Chivier Humber">
  <meta name="keywords" content="">
  
    <meta name="description" content="1 Requirements">
<meta property="og:type" content="article">
<meta property="og:title" content="2202-CudaProgramming">
<meta property="og:url" content="https://chivier.github.io/2022/02/20/2022/2202-CudaProgramming/index.html">
<meta property="og:site_name" content="Chivier&#39;s Blog">
<meta property="og:description" content="1 Requirements">
<meta property="og:locale">
<meta property="og:image" content="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/graphics/nvcc-options-for-separate-compilation.png">
<meta property="og:image" content="https://chivier.github.io/2022/02/20/2022/2202-CudaProgramming/image1.png">
<meta property="og:image" content="https://chivier.github.io/2022/02/20/2022/2202-CudaProgramming/image2.png">
<meta property="og:image" content="https://chivier.github.io/2022/02/20/2022/2202-CudaProgramming/image3.png">
<meta property="og:image" content="https://chivier.github.io/2022/02/20/2022/2202-CudaProgramming/image4.png">
<meta property="og:image" content="https://chivier.github.io/2022/02/20/2022/2202-CudaProgramming/image5.png">
<meta property="article:published_time" content="2022-02-19T22:28:54.000Z">
<meta property="article:modified_time" content="2022-07-10T12:09:43.909Z">
<meta property="article:author" content="Chivier Humber">
<meta property="article:tag" content="Cuda">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/graphics/nvcc-options-for-separate-compilation.png">
  
  
  
  <title>2202-CudaProgramming - Chivier&#39;s Blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"chivier.github.io","root":"/","version":"1.9.2","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 5.4.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Chivier</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                Categories
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Tags
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="2202-CudaProgramming"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2022-02-20 06:28" pubdate>
          February 20, 2022 am
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          22k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          183 mins
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">2202-CudaProgramming</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="1-Requirements"><a href="#1-Requirements" class="headerlink" title="1 Requirements"></a>1 Requirements</h1><span id="more"></span>


<p>在开始教程之前，简单说明一下下面步骤的需求和测试方法：</p>
<ol>
<li>Git: 之后的范例使用 git 作为拉取</li>
<li>Cuda 11+：使用 11 以上的 cuda 版本确保之后的步骤可以正常使用</li>
<li>CMake：项目构建采用 cmake 确保多平台可以进行测试和实验</li>
</ol>
<p>官方文档的学习曲线比较陡峭，下面整理一些例子帮助快速上手。<br>参考官方手册：Cuda Programming Guide，见附件。此后简称它“手册”。</p>
<h1 id="2-设备和主机"><a href="#2-设备和主机" class="headerlink" title="2 设备和主机"></a>2 设备和主机</h1><p>首先在这里，我个人想赞美一下 Nvidia 作为国际一流大厂的开源精神和优秀兼容性。现在的 Cuda 支持跨平台、语法兼容。这里语法兼容是指：cuda 是包含 C++17 语法的。直接写的 C++ 代码是可以在机器上直接运行的。</p>
<p>官方也有一个例子，说明 cuda 和 g++的编译内容可以混合使用：<br><img src="https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/graphics/nvcc-options-for-separate-compilation.png" srcset="/img/loading.gif" lazyload alt="Flow diagram of nvcc options for separate compilation"><br>这一设计大大节约了程序移植的时间。</p>
<p>在开始编程之前，需要分离我们的传统思维，程序可以不只在 CPU 上运行，还可以在 GPU 上面运行。因此，这里诞生两个概念：Host &amp; Device。<br>实际上，对于大部分的异构计算框架，例如：OpenCL、UPC 等等，都是可以指定 Host &amp; Device 的。这里的 Host 一般代指我们的 CPU, 而 Device 代指我们的 GPU。<br>下面是一个例子：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs cpp">  <br><span class="hljs-function">__device__ <span class="hljs-keyword">void</span> <span class="hljs-title">gpu_hello</span><span class="hljs-params">()</span> </span>&#123;  <br>   <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;gpu hello!\n&quot;</span>);  <br>&#125;  <br>  <br><span class="hljs-function">__host__ <span class="hljs-keyword">void</span> <span class="hljs-title">cpu_hello</span><span class="hljs-params">()</span> </span>&#123;  <br>   <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;cpu hello!\n&quot;</span>);  <br>&#125;  <br>  <br><span class="hljs-function">__global__ <span class="hljs-keyword">void</span> <span class="hljs-title">kernel</span><span class="hljs-params">()</span> </span>&#123;  <br>   gpu_hello();  <br>&#125;  <br>  <br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;  <br>   kernel&lt;&lt;&lt;<span class="hljs-number">1</span>, <span class="hljs-number">2</span>&gt;&gt;&gt;();  <br>   cudaDeviceSynchronize();  <br>   cpu_hello();  <br>   <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;  <br>&#125;<br></code></pre></td></tr></table></figure>

<p>其中我们可以看到三类不同的函数修饰：<br>_<em>device_<em>，__host__，和 __global\</em></em></p>
<p>在手册 4.2.1 函数类型限定词一节中进行了详细介绍。</p>
<h2 id="2-1-设备定义"><a href="#2-1-设备定义" class="headerlink" title="2.1 设备定义"></a>2.1 设备定义</h2><ul>
<li>_<em>device_</em> 是在设备上跑的，<strong>只可以</strong>从设备上调用</li>
<li>_<em>host_</em> 是在 CPU 上跑的，<strong>只可以</strong>从主机上调用</li>
<li>_<em>global_</em> 是在设备商跑的，<strong>只可以</strong>从主机上调用</li>
</ul>
<img src="image1.png" srcset="/img/loading.gif" lazyload width="80%" height="80%">

<p>如果一个函数不加修饰，默认他是 _<em>device_</em> 函数，正如上面的 main 一样。</p>
<p>如果一个函数需要同时在 CPU 和 GPU 上都能执行，那么可以同时加上 host 和 device 两个关键字。<br>需要判断具体在那个设备上，可以使用：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-comment">// cuda</span><br><span class="hljs-comment">// cpu</span><br></code></pre></td></tr></table></figure>

<h2 id="2-2-函数调用"><a href="#2-2-函数调用" class="headerlink" title="2.2 函数调用"></a>2.2 函数调用</h2><p>如果是 device 或者 host 函数，我们可以在恰当的位置直接调用。<br>如果是 global 调用的时候，我们需要用&lt;&lt;&lt;arg1, arg2&gt;&gt;&gt;，去声明我们申请的资源。这个我们在[下一节](#\ 3\ Block\ 和\ Thread) 会说明。</p>
<h2 id="2-3-函数限制"><a href="#2-3-函数限制" class="headerlink" title="2.3 函数限制"></a>2.3 函数限制</h2><p>由于计算的行为限制，一些特殊的程序行为在 cuda 代码中是被严格禁止的，例如：</p>
<ol>
<li>_<em>host_</em> 和 _<em>global_</em> 不支持递归</li>
<li>_<em>global_</em> 返回值要求是 void</li>
<li>调用 GPU 的函数声明和定义不要分离，写在同一个文件里</li>
</ol>
<p>更多限制见手册，不过上述两条基本包含了日常开发的需要。</p>
<p>这里额外补充一点，是因为特殊的需要说明一下。<br>关于上述的第三点，我们尽管可以使用特殊的方法，例如CUDA_SEPARABLE_COMPILATION等方法可以分离定义，但是会有无法内联等问题对性能产生巨大影响。处于这些原因，我个人建议<strong>global__和__device</strong> <strong><u>声明和调用他们的地方</u></strong> 最好写到同一个文件里。</p>
<h2 id="2-4-Cuda-Version-amp-GPU-Version"><a href="#2-4-Cuda-Version-amp-GPU-Version" class="headerlink" title="2.4 Cuda Version &amp; GPU Version"></a>2.4 Cuda Version &amp; GPU Version</h2><p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/CUDA">https://en.wikipedia.org/wiki/CUDA</a></p>
<p>这个部分一般来说不太需要关心，但是上面给出一个链接用来说明各个版本的 GPU 的 compute capacity 的问题，每张 GPU 都有一个属于自己的版本号，版本号是向下兼容的，即高版本号的卡可以跑任何版本号低于自己卡上编译出来的程序。用尽可能高版本的卡的特性去编译程序有利于充分发挥程序的性能。</p>
<p>在 <code>CMakeLists.txt</code> 中设置：</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake"><span class="hljs-keyword">set</span> (CMAKE_CUDA_ARCHITECTURES <span class="hljs-number">61</span>)<br></code></pre></td></tr></table></figure>

<p>其中 61 是 1080 的版本号，在表格中有写出，也可以使用程序输出：</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs cmake">  <br>__device__ void gpu_hello() &#123;  <br>   <span class="hljs-comment">#ifdef __CUDA_ARCH__  </span><br>       printf(<span class="hljs-string">&quot;%d\n&quot;</span>, __CUDA_ARCH__);  <br>   <span class="hljs-comment">#endif  </span><br>&#125;  <br>  <br>__global__ void kernel() &#123;  <br>   gpu_hello();  <br>&#125;  <br>  <br>int main() &#123;  <br>   kernel&lt;&lt;&lt;<span class="hljs-number">1</span>, <span class="hljs-number">1</span>&gt;&gt;&gt;();  <br>   cudaDeviceSynchronize();  <br>   <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;  <br>&#125;<br></code></pre></td></tr></table></figure>
<p>用来检测当前的版本是否正确，因为如果在 <code>CMakeLists.txt</code> 中不指明，那么机器会默认使用最低版本的驱动，也就是 520 版本的进行编译和后续优化。这显然不是我们希望的。</p>
<p>这里有一个我之前收藏的小文，介绍了更多的硬件细节：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/394352476">https://zhuanlan.zhihu.com/p/394352476</a></p>
<h2 id="2-5-关于设备上的输出问题"><a href="#2-5-关于设备上的输出问题" class="headerlink" title="2.5 关于设备上的输出问题"></a>2.5 关于设备上的输出问题</h2><p>如果我们在 device 或者 global 的代码里调用了 printf 等输出函数，这个时候需要使用前面已经多次使用过的 <code>cudaDeviceSynchronize()</code> 才能生效。因为处于高性能的需求，我们的 CPU 代码执行和 GPU 代码执行是异步执行的。所以我们调用 GPU 代码之后，CPU 程序实际上是会继续执行的。如果要等上面的 GPU 代码执行完成，需要在此处同步等待一下。<br>（可以尝试把之前程序的 <code>cudaDeviceSynchronize()</code> 删除进行一个小小的测试）<br>此外 cout 等流输出在 cuda 中是禁止的，因为流输出里包含了过多的函数行为和复杂特性，Nvidia 的工程师暂时还没有实现这些特性。</p>
<h1 id="3-Block-和-Thread"><a href="#3-Block-和-Thread" class="headerlink" title="3 Block 和 Thread"></a>3 Block 和 Thread</h1><p>上面一节我们提到，如果我们调用 global 声明的函数的时候需要说明计算资源。这里我们解释一下 &lt;&lt;&lt;arg1, arg2&gt;&gt;&gt; 中两个参数的含义。</p>
<h2 id="3-1-从例子开始"><a href="#3-1-从例子开始" class="headerlink" title="3.1 从例子开始"></a>3.1 从例子开始</h2><p>首先先从下面这个例子说明：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs cpp">  <br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> <span class="hljs-built_in">std</span>;  <br><span class="hljs-function">__global__ <span class="hljs-keyword">void</span> <span class="hljs-title">kernel</span><span class="hljs-params">()</span> </span>&#123;  <br>   <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;Block %d of %d, Thread %d of %d\n&quot;</span>, blockIdx.x, gridDim.x, threadIdx.x, blockDim.x);  <br>&#125;  <br>  <br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;  <br>   kernel&lt;&lt;&lt;<span class="hljs-number">4</span>, <span class="hljs-number">3</span>&gt;&gt;&gt;();  <br>   cudaDeviceSynchronize();  <br>   <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;  <br>&#125;<br></code></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs text">Block 0 of 4, Thread 0 of 3  <br>Block 0 of 4, Thread 1 of 3  <br>Block 0 of 4, Thread 2 of 3  <br>Block 2 of 4, Thread 0 of 3  <br>Block 2 of 4, Thread 1 of 3  <br>Block 2 of 4, Thread 2 of 3  <br>Block 1 of 4, Thread 0 of 3  <br>Block 1 of 4, Thread 1 of 3  <br>Block 1 of 4, Thread 2 of 3  <br>Block 3 of 4, Thread 0 of 3  <br>Block 3 of 4, Thread 1 of 3  <br>Block 3 of 4, Thread 2 of 3<br></code></pre></td></tr></table></figure>

<p>Cuda 有两个层级的并行，一个是 block 级别，一个是 thread 级别。<br>&lt;&lt;&lt;arg1, arg2&gt;&gt;&gt; 中：</p>
<ul>
<li>Arg1 = block 数目 = <code>gridDim.x</code></li>
<li>Arg2 = 每个 block 中 thread 的数目 = <code>blockDim.x</code><br>（我曾经反对首字母小写的驼峰命名，后来被 Nvidia 教育了）</li>
</ul>
<h2 id="3-2-Block-和-Thread-的设计意义"><a href="#3-2-Block-和-Thread-的设计意义" class="headerlink" title="3.2 Block 和 Thread 的设计意义"></a>3.2 Block 和 Thread 的设计意义</h2><p>为什么需要进行这两级的设计呢？<br>其实这个和 GPU 的硬件设计方式有关系。我们的代码直接和硬件设计相关。</p>
<p>首先从命名上不难看出，GPU 的设计是为了进行 2D 和 3D 的网格类型计算。对应的应用就是：图形图像处理、粒子引擎模拟、网格算法计算。我们在计算是经常需要 “切分大数据，进行分发计算，最后再进行收集”。也就是常说的 Scatter-and-Gather, 或者 Fork-and-Merge。</p>
<p>现在的 GPU 架构中：</p>
<ul>
<li>一个 GPU = 多个 Streaming Multiprocessor (SM) + cache 组成</li>
<li>一个 SM = Streaming Processor（SP）+ cache 组成</li>
<li>SM 用于处理 block</li>
<li>SP 用于处理 thread</li>
</ul>
<p>很大一方面理由就是为了更加充分利用多级缓存的优势。</p>
<h2 id="3-3-Flatten-Mode-和-Dimension-Mode"><a href="#3-3-Flatten-Mode-和-Dimension-Mode" class="headerlink" title="3.3 Flatten Mode 和 Dimension Mode"></a>3.3 Flatten Mode 和 Dimension Mode</h2><p>由于 Block 和 Thread 的两级设计导致我们的划分可能存在一定的困难，有一个简单粗暴的方法可以直接把他们压平。直接把上面那个例子进行压平改写：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs cpp">  <br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> <span class="hljs-built_in">std</span>;  <br><span class="hljs-function">__global__ <span class="hljs-keyword">void</span> <span class="hljs-title">kernel</span><span class="hljs-params">()</span> </span>&#123;  <br>   <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;Thread %d of %d\n&quot;</span>, blockIdx.x * blockDim.x + threadIdx.x, blockDim.x * gridDim.x);  <br>&#125;  <br>  <br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;  <br>   kernel&lt;&lt;&lt;<span class="hljs-number">4</span>, <span class="hljs-number">3</span>&gt;&gt;&gt;();  <br>   cudaDeviceSynchronize();  <br>   <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;  <br>&#125;<br></code></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs text">Thread 0 of 12  <br>Thread 1 of 12  <br>Thread 2 of 12  <br>Thread 6 of 12  <br>Thread 7 of 12  <br>Thread 8 of 12  <br>Thread 3 of 12  <br>Thread 4 of 12  <br>Thread 5 of 12  <br>Thread 9 of 12  <br>Thread 10 of 12  <br>Thread 11 of 12<br></code></pre></td></tr></table></figure>

<p>由于 cuda 的适用范围，还有另外一种模式的其实在 HPC 应用中更加受到欢迎和广泛使用，一般称为 Dimension Mode。</p>
<p>首先展示一下结果：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs cpp">  <br><span class="hljs-function">__global__ <span class="hljs-keyword">void</span> <span class="hljs-title">kernel</span><span class="hljs-params">()</span> </span>&#123;  <br>   <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;Block (%d,%d,%d) of (%d,%d,%d), Thread (%d,%d,%d) of (%d,%d,%d)\n&quot;</span>,  <br>          blockIdx.x, blockIdx.y, blockIdx.z,  <br>          gridDim.x, gridDim.y, gridDim.z,  <br>          threadIdx.x, threadIdx.y, threadIdx.z,  <br>          blockDim.x, blockDim.y, blockDim.z);  <br>&#125;  <br>  <br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;  <br>   kernel&lt;&lt;&lt;dim3(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>), dim3(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)&gt;&gt;&gt;();  <br>   cudaDeviceSynchronize();  <br>   <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;  <br>&#125;<br></code></pre></td></tr></table></figure>

<p>首先看三维的例子：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs text">Block (0,0,0) of (2,1,1), Thread (0,0,0) of (2,2,2)  <br>Block (0,0,0) of (2,1,1), Thread (1,0,0) of (2,2,2)  <br>Block (0,0,0) of (2,1,1), Thread (0,1,0) of (2,2,2)  <br>Block (0,0,0) of (2,1,1), Thread (1,1,0) of (2,2,2)  <br>Block (0,0,0) of (2,1,1), Thread (0,0,1) of (2,2,2)  <br>Block (0,0,0) of (2,1,1), Thread (1,0,1) of (2,2,2)  <br>Block (0,0,0) of (2,1,1), Thread (0,1,1) of (2,2,2)  <br>Block (0,0,0) of (2,1,1), Thread (1,1,1) of (2,2,2)  <br>Block (1,0,0) of (2,1,1), Thread (0,0,0) of (2,2,2)  <br>Block (1,0,0) of (2,1,1), Thread (1,0,0) of (2,2,2)  <br>Block (1,0,0) of (2,1,1), Thread (0,1,0) of (2,2,2)  <br>Block (1,0,0) of (2,1,1), Thread (1,1,0) of (2,2,2)  <br>Block (1,0,0) of (2,1,1), Thread (0,0,1) of (2,2,2)  <br>Block (1,0,0) of (2,1,1), Thread (1,0,1) of (2,2,2)  <br>Block (1,0,0) of (2,1,1), Thread (0,1,1) of (2,2,2)  <br>Block (1,0,0) of (2,1,1), Thread (1,1,1) of (2,2,2)<br></code></pre></td></tr></table></figure>

<p>再来给出一个二维的例子：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs cpp">  <br><span class="hljs-function">__global__ <span class="hljs-keyword">void</span> <span class="hljs-title">kernel</span><span class="hljs-params">()</span> </span>&#123;  <br>   <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;Block (%d,%d,%d) of (%d,%d,%d), Thread (%d,%d,%d) of (%d,%d,%d)\n&quot;</span>,  <br>          blockIdx.x, blockIdx.y, blockIdx.z,  <br>          gridDim.x, gridDim.y, gridDim.z,  <br>          threadIdx.x, threadIdx.y, threadIdx.z,  <br>          blockDim.x, blockDim.y, blockDim.z);  <br>&#125;  <br>  <br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;  <br>   kernel&lt;&lt;&lt;dim3(<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>), dim3(<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)&gt;&gt;&gt;();  <br>   cudaDeviceSynchronize();  <br>   <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;  <br>&#125;<br></code></pre></td></tr></table></figure>

<p>结果最后一个维度都是 0, 我们使用结果的时候不使用 z 维度即可</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs text">Block (1,2,0) of (2,3,1), Thread (0,0,0) of (2,1,1)  <br>Block (1,2,0) of (2,3,1), Thread (1,0,0) of (2,1,1)  <br>Block (0,2,0) of (2,3,1), Thread (0,0,0) of (2,1,1)  <br>Block (0,2,0) of (2,3,1), Thread (1,0,0) of (2,1,1)  <br>Block (0,1,0) of (2,3,1), Thread (0,0,0) of (2,1,1)  <br>Block (0,1,0) of (2,3,1), Thread (1,0,0) of (2,1,1)  <br>Block (1,0,0) of (2,3,1), Thread (0,0,0) of (2,1,1)  <br>Block (1,0,0) of (2,3,1), Thread (1,0,0) of (2,1,1)  <br>Block (0,0,0) of (2,3,1), Thread (0,0,0) of (2,1,1)  <br>Block (0,0,0) of (2,3,1), Thread (1,0,0) of (2,1,1)  <br>Block (1,1,0) of (2,3,1), Thread (0,0,0) of (2,1,1)  <br>Block (1,1,0) of (2,3,1), Thread (1,0,0) of (2,1,1)<br></code></pre></td></tr></table></figure>

<h2 id="3-4-如何设置计算资源？"><a href="#3-4-如何设置计算资源？" class="headerlink" title="3.4 如何设置计算资源？"></a>3.4 如何设置计算资源？</h2><p>上面说了我们 GPU 并行的时候有两个层级，第一个是 Block 级别，第二个是 Thread 级别。为此我们需要设置两个级别的数目。</p>
<p>首先我们先看我们最多可以用多少资源。使用 cuda samples 中的 <a target="_blank" rel="noopener" href="https://github.com/nvidia/cuda-samples">deviceQuery</a> （Make 直接编译整个 cuda-samples 项目可以找到）</p>
<img src="image2.png" srcset="/img/loading.gif" lazyload width="80%" height="80%">

<p>此程序会列举机器上的所有设备，列举设备之后可以展示出机器的所有详细信息。这里我们看到了关键的几行信息：</p>
<p>Maximum number of threads per block<br>以及<br>Maximum dimension size of a thread block</p>
<p>前者比较好理解，我们看看后一个 dimension 的含义，首先先列举一下<a target="_blank" rel="noopener" href="https://forums.developer.nvidia.com/t/maximum-number-of-threads-on-thread-block/46392/2">官方论坛</a>的说辞：</p>
<blockquote>
<p>There are multiple limits. All must be satisfied.</p>
<ol>
<li> The maximum number of threads in the block is limited to 1024. This is the product of whatever your threadblock dimensions are (x_y_z). For example (32,32,1) creates a block of 1024 threads. (33,32,1) is not legal, since <code>33*32*1</code> &gt; 1024.</li>
<li> The maximum x-dimension is 1024. (1024, 1, 1) is legal. (1025, 1, 1) is not legal.</li>
<li> The maximum y-dimension is 1024. (1, 1024, 1) is legal. (1, 1025, 1) is not legal.</li>
<li> The maximum z-dimension is 64. (1, 1, 64) is legal. (2, 2, 64) is also legal. (1, 1, 65) is not legal.</li>
</ol>
</blockquote>
<p>配合论坛中的内容，这里也很好理解了。</p>
<p>结合了上面的例子，cuda 的变成已经变得相对直观，用来优化一些简单的循环已经可以实现了。这里可以概括一下：</p>
<ol>
<li>cuda Block 级别相当于 C++ 线程，数目可以设置比较大，调度依靠 GPU ，方式类似于 CPU 调度 threads</li>
<li>cuda Thread 级别相当于 SIMD，有数目上限，受限于 cuda core 的数目和一些维度参数</li>
</ol>
<h1 id="4-变量行为"><a href="#4-变量行为" class="headerlink" title="4 变量行为"></a>4 变量行为</h1><p>简单的例子说明完成，下面开始介绍每一个具体的函数内部，我们程序的行为。</p>
<h2 id="4-1-显存和内存"><a href="#4-1-显存和内存" class="headerlink" title="4.1 显存和内存"></a>4.1 显存和内存</h2><p>在使用 GPU 编程（也有可能在玩游戏）的时候，有一个词时常出现：显存。<br>这个词已经暗示我们 GPU 的内存分布和 CPU 的内存分布实际上是不一样的。我们在设备上的内容、和主机上的内容直接需要一些同步机制去同步。</p>
<p>首先一图概览接下来的内容：</p>
<img src="image3.png" srcset="/img/loading.gif" lazyload width="80%" height="80%">

<p>简述一下：</p>
<ol>
<li>显存和内存独立管理各自的 DRAM</li>
<li>互相调用树枝需要使用 cudaMemcpy</li>
<li>Unified 统一管理模式可以解决管理问题，但是代价是需要额外的时间开销</li>
</ol>
<p>下面配合具体的例子说明。</p>
<h2 id="4-2-显存内存不能互相调用"><a href="#4-2-显存内存不能互相调用" class="headerlink" title="4.2 显存内存不能互相调用"></a>4.2 显存内存不能互相调用</h2><h3 id="4-2-1-设备调用内存失败"><a href="#4-2-1-设备调用内存失败" class="headerlink" title="4.2.1 设备调用内存失败"></a>4.2.1 设备调用内存失败</h3><p>我们复习一下在内存中如何申请空间，在 C/C++ 中，我们一般会使用一对函数： malloc &amp; free 用于一块内存的申请与释放。</p>
<p>下面给出一个例子：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs cpp">  <br><span class="hljs-function">__global__ <span class="hljs-keyword">void</span> <span class="hljs-title">kernel</span><span class="hljs-params">(<span class="hljs-keyword">int</span> *arr)</span> </span>&#123;  <br>   arr[<span class="hljs-number">0</span>] = <span class="hljs-number">0</span>;  <br>   <span class="hljs-keyword">int</span> index = <span class="hljs-number">1</span>;  <br>   <span class="hljs-keyword">while</span> (arr[index] != <span class="hljs-number">0</span>) &#123;  <br>       arr[<span class="hljs-number">0</span>] += arr[index];  <br>       index++;  <br>   &#125;  <br>&#125;  <br>  <br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;  <br>   <span class="hljs-keyword">int</span> *a;  <br>   a = (<span class="hljs-keyword">int</span> *)<span class="hljs-built_in">malloc</span>(<span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">int</span>) * <span class="hljs-number">12</span>);  <br>   <span class="hljs-keyword">int</span> index = <span class="hljs-number">1</span>;  <br>   <span class="hljs-keyword">for</span> (index = <span class="hljs-number">1</span>; index &lt;= <span class="hljs-number">10</span>; ++index) &#123;  <br>       a[index] = index;  <br>   &#125;  <br>   kernel&lt;&lt;&lt;<span class="hljs-number">1</span>, <span class="hljs-number">1</span>&gt;&gt;&gt;(a);  <br>   <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%d&quot;</span>, a[<span class="hljs-number">0</span>]);  <br>   cudaDeviceSynchronize();  <br>   <span class="hljs-built_in">free</span>(a);<br>   <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;  <br>&#125;<br></code></pre></td></tr></table></figure>

<h3 id="4-2-2-主机调用显存失败"><a href="#4-2-2-主机调用显存失败" class="headerlink" title="4.2.2 主机调用显存失败"></a>4.2.2 主机调用显存失败</h3><p>这里首先说明一下 cuda 是如何展示显存的。<br>在介绍之前，先需要介绍一下两个函数接口：cudaMalloc &amp; cudaFree</p>
<p>这里需要说明的是 cudaMalloc 的用法和 malloc 差异极大：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c"><span class="hljs-function">cudaError_t <span class="hljs-title">cudaMalloc</span> <span class="hljs-params">(<span class="hljs-keyword">void</span>** ptr, <span class="hljs-keyword">size_t</span> size)</span></span>;<br></code></pre></td></tr></table></figure>

<p>配合下图理解：</p>
<img src="image4.png" srcset="/img/loading.gif" lazyload width="80%" height="80%">

<p>我们申请的是一个 “指向 GPU 数据指针的指针”。</p>
<p>我们申请到的空间是在一个二级指针上存储，下面举一个例子：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs cpp">  <br><span class="hljs-function">__global__ <span class="hljs-keyword">void</span> <span class="hljs-title">kernel</span><span class="hljs-params">(<span class="hljs-keyword">int</span> *arr)</span> </span>&#123;  <br>   arr[<span class="hljs-number">0</span>] = <span class="hljs-number">0</span>;  <br>   <span class="hljs-keyword">int</span> index = <span class="hljs-number">1</span>;  <br>   <span class="hljs-keyword">while</span> (arr[index] != <span class="hljs-number">0</span>) &#123;  <br>       arr[<span class="hljs-number">0</span>] += arr[index];  <br>       index++;  <br>   &#125;  <br>&#125;  <br>  <br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;  <br>   <span class="hljs-keyword">int</span> *a;  <br>   cudaMalloc(&amp;a, <span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">int</span>) * <span class="hljs-number">12</span>);  <br>   <span class="hljs-keyword">int</span> index = <span class="hljs-number">1</span>;  <br>   <span class="hljs-keyword">for</span> (index = <span class="hljs-number">1</span>; index &lt;= <span class="hljs-number">10</span>; ++index) &#123;  <br>       a[index] = index;  <br>   &#125;  <br>   kernel&lt;&lt;&lt;<span class="hljs-number">1</span>, <span class="hljs-number">1</span>&gt;&gt;&gt;(a);  <br>   <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%d&quot;</span>, a[<span class="hljs-number">0</span>]);  <br>   cudaDeviceSynchronize();  <br>   cudaFree(a);<br>   <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;  <br>&#125;<br></code></pre></td></tr></table></figure>

<h3 id="4-2-3-cuda-Error-Handling"><a href="#4-2-3-cuda-Error-Handling" class="headerlink" title="4.2.3 cuda Error Handling"></a>4.2.3 cuda Error Handling</h3><p>上面两个失败的例子再次说明了内存和显存不能“直接”混合使用。（下面会说一种支持混合使用的技术）<br>前者结果是 0, 后者结果是段错误。导致的原因是因为内存地址和显存地址不是直接统一的。</p>
<p>为了方便调试，cuda 提供了一系列的纠错调试机制，在前面的程序中，用到的：<code>cudaMalloc</code>，<code>cudaFree</code>，<code>cudaDeviceSynchronize</code> 都有返回类型便于我们调试，返回类型是一个 <code>cudaError_t</code> 的没枚举类型，可以被 printf 直接输出。</p>
<p>但是输出了我们是不能理解函数的具体含义的，也不方便我们进行 debug。这里 cuda samples 里面提供了一个非常好的例子，我们可以直接引用 <code>helper_cuda.h</code> 头文件解决问题。</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs cmake"><span class="hljs-keyword">cmake_minimum_required</span>(VERSION <span class="hljs-number">3.10</span>)  <br>  <br><span class="hljs-keyword">set</span>(CMAKE_CXX_STANDARD <span class="hljs-number">17</span>)  <br><span class="hljs-keyword">set</span>(CMAKE_BUILD_TYPE Release)  <br><span class="hljs-keyword">set</span>(CMAKE_CUDA_ARCHITECTURES <span class="hljs-number">61</span>)  <br>  <br><span class="hljs-keyword">project</span>(devices LANGUAGES CXX CUDA)  <br>  <br><span class="hljs-keyword">add_executable</span>(hello hello.cu)  <br><span class="hljs-keyword">target_include_directories</span>(hello PUBLIC /usr/local/cuda/samples/common/inc)<br></code></pre></td></tr></table></figure>

<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs cpp">  <br><span class="hljs-function">__global__ <span class="hljs-keyword">void</span> <span class="hljs-title">kernel</span><span class="hljs-params">(<span class="hljs-keyword">int</span> *arr)</span> </span>&#123;  <br>   arr[<span class="hljs-number">0</span>] = <span class="hljs-number">0</span>;  <br>   <span class="hljs-keyword">int</span> index = <span class="hljs-number">1</span>;  <br>   <span class="hljs-keyword">while</span> (arr[index] != <span class="hljs-number">0</span>) &#123;  <br>       arr[<span class="hljs-number">0</span>] += arr[index];  <br>       index++;  <br>   &#125;  <br>&#125;  <br>  <br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;  <br>   <span class="hljs-keyword">int</span> *a;  <br>   a = (<span class="hljs-keyword">int</span> *)<span class="hljs-built_in">malloc</span>(<span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">int</span>) * <span class="hljs-number">12</span>);  <br>   <span class="hljs-keyword">int</span> index = <span class="hljs-number">1</span>;  <br>   <span class="hljs-keyword">for</span> (index = <span class="hljs-number">1</span>; index &lt;= <span class="hljs-number">10</span>; ++index) &#123;  <br>       a[index] = index;  <br>   &#125;  <br>   kernel&lt;&lt;&lt;<span class="hljs-number">1</span>, <span class="hljs-number">1</span>&gt;&gt;&gt;(a);  <br>   checkCudaErrors(cudaDeviceSynchronize());  <br>   <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%d\n&quot;</span>, a[<span class="hljs-number">0</span>]);  <br>   <span class="hljs-built_in">free</span>(a);  <br>   <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;  <br>&#125;<br></code></pre></td></tr></table></figure>

<p>这样就完成了我们的 debug 工作，此时输出结果为：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs text">0<br>CUDA error at /home/chivier/Projects/cudatest/08-errhandle/hello.cu:23 code=700(cudaErrorIllegalAddress) &quot;cudaDeviceSynchronize()&quot;<br></code></pre></td></tr></table></figure>

<p>这里的 IllegalAddress 就说明了地址无法正常使用。</p>
<h3 id="4-2-5-cudaMemcpy"><a href="#4-2-5-cudaMemcpy" class="headerlink" title="4.2.5 cudaMemcpy"></a>4.2.5 cudaMemcpy</h3><p>那么正确的做法是什么呢？我们如果需要通信两个地方的内存，需要借助 cudaMemcpy 函数。</p>
<p>下面是正确的改正方法：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs cpp">  <br><span class="hljs-function">__global__ <span class="hljs-keyword">void</span> <span class="hljs-title">kernel</span><span class="hljs-params">(<span class="hljs-keyword">int</span> *arr)</span> </span>&#123;  <br>   arr[<span class="hljs-number">0</span>] = <span class="hljs-number">0</span>;  <br>   <span class="hljs-keyword">int</span> index = <span class="hljs-number">1</span>;  <br>   <span class="hljs-keyword">while</span> (arr[index] != <span class="hljs-number">0</span>) &#123;  <br>       arr[<span class="hljs-number">0</span>] += arr[index];  <br>       index++;  <br>   &#125;  <br>&#125;  <br>  <br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;  <br>   <span class="hljs-keyword">int</span> *a;  <br>   a = (<span class="hljs-keyword">int</span> *)<span class="hljs-built_in">malloc</span>(<span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">int</span>) * <span class="hljs-number">12</span>);  <br>   <span class="hljs-keyword">int</span> index = <span class="hljs-number">1</span>;  <br>   <span class="hljs-keyword">for</span> (index = <span class="hljs-number">1</span>; index &lt;= <span class="hljs-number">10</span>; ++index) &#123;  <br>       a[index] = index;  <br>   &#125;  <br>  <br>   <span class="hljs-keyword">int</span> *cuda_a;  <br>   cudaMalloc(&amp;cuda_a, <span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">int</span>) * <span class="hljs-number">12</span>);  <br>   cudaMemcpy(cuda_a, a, <span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">int</span>) * <span class="hljs-number">12</span>, cudaMemcpyHostToDevice);  <br>   kernel&lt;&lt;&lt;<span class="hljs-number">1</span>, <span class="hljs-number">1</span>&gt;&gt;&gt;(cuda_a);  <br>   cudaMemcpy(a, cuda_a, <span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">int</span>) * <span class="hljs-number">12</span>, cudaMemcpyDeviceToHost);  <br>  <br>   checkCudaErrors(cudaDeviceSynchronize());  <br>   <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%d\n&quot;</span>, a[<span class="hljs-number">0</span>]);  <br>   <span class="hljs-built_in">free</span>(a);  <br>   cudaFree(cuda_a);  <br>   <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;  <br>&#125;<br></code></pre></td></tr></table></figure>

<p>结果是：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs text">55<br></code></pre></td></tr></table></figure>

<p>这个时候我们可以总结一般的 cuda 变成行为方法：</p>
<ol>
<li>Generate Data/Read Data</li>
<li>Copy: Host-&gt;Device</li>
<li>Calculate</li>
<li>Copy: Device-&gt;Host</li>
<li>Print out</li>
</ol>
<p>这里说明一下：<br>cudaMemcpy可以自动实现同步工作，可以省去cudaDeviceSynchronize。</p>
<h3 id="4-2-4-Unified-Memory"><a href="#4-2-4-Unified-Memory" class="headerlink" title="4.2.4 Unified Memory"></a>4.2.4 Unified Memory</h3><p>首先先给出官方的一份博客：<a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/unified-memory-in-cuda-6/">Unified Memory</a></p>
<p>我们把上面的例子直接改写：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs cpp">  <br><span class="hljs-function">__global__ <span class="hljs-keyword">void</span> <span class="hljs-title">kernel</span><span class="hljs-params">(<span class="hljs-keyword">int</span> *arr)</span> </span>&#123;  <br>   arr[<span class="hljs-number">0</span>] = <span class="hljs-number">0</span>;  <br>   <span class="hljs-keyword">int</span> index = <span class="hljs-number">1</span>;  <br>   <span class="hljs-keyword">while</span> (arr[index] != <span class="hljs-number">0</span>) &#123;  <br>       arr[<span class="hljs-number">0</span>] += arr[index];  <br>       index++;  <br>   &#125;  <br>&#125;  <br>  <br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;  <br>   <span class="hljs-keyword">int</span> *a;  <br>   checkCudaErrors(cudaMallocManaged(&amp;a, <span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">int</span>) * <span class="hljs-number">12</span>));  <br>   <span class="hljs-keyword">int</span> index = <span class="hljs-number">1</span>;  <br>   <span class="hljs-keyword">for</span> (index = <span class="hljs-number">1</span>; index &lt;= <span class="hljs-number">10</span>; ++index) &#123;  <br>       a[index] = index;  <br>   &#125;  <br>  <br>   kernel&lt;&lt;&lt;<span class="hljs-number">1</span>, <span class="hljs-number">1</span>&gt;&gt;&gt;(a);  <br>      <br>   checkCudaErrors(cudaDeviceSynchronize());  <br>   <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%d\n&quot;</span>, a[<span class="hljs-number">0</span>]);  <br>   cudaFree(a);  <br>   <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;  <br>&#125;<br></code></pre></td></tr></table></figure>

<p>Unified Memory 申请的空间在 Host 和 Device 上都直接可以正常使用。节省代码，但是代价也是随之而来的，我们需要使用一定的性能作为代价。这个有时是劣势，但有些时候这种方法反而可以达到优化的效果。</p>
<h2 id="4-3-变量传输和计算"><a href="#4-3-变量传输和计算" class="headerlink" title="4.3 变量传输和计算"></a>4.3 变量传输和计算</h2><p>这里我们简单介绍一下一般的循环模式。</p>
<h3 id="4-3-1-grid-stride-loop"><a href="#4-3-1-grid-stride-loop" class="headerlink" title="4.3.1 grid-stride loop"></a>4.3.1 grid-stride loop</h3><p>在 cuda 中，并行的模式有一点特殊，我们称为 grid-stride loop。</p>
<p>简单解释一下：</p>
<p>如果我们希望对一个循环进行并行，直观的想法就是分配足够多的线程执行他。如果循环次数少于 1024, 我们完全可以使用一个 block, 里面分配 <u>循环次数对应的线程数</u> 从而达到目的。但是这里不是我们希望看到的结果，因为我们不能保证我们的循环次数一定小于 1024。我们用下面的例子说明我们是如何用 cuda 执行循环的：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs cpp">  <br><span class="hljs-function">__global__ <span class="hljs-keyword">void</span> <span class="hljs-title">kernel</span><span class="hljs-params">(<span class="hljs-keyword">int</span> *arr, <span class="hljs-keyword">int</span> n)</span> </span>&#123;  <br>   <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = threadIdx.x; i &lt; n; i += blockDim.x) &#123;  <br>       arr[i] = i;  <br>   &#125;  <br>&#125;  <br>  <br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;  <br>   <span class="hljs-keyword">int</span> n = <span class="hljs-number">114</span>;  <br>   <span class="hljs-keyword">int</span> *arr;  <br>   checkCudaErrors(cudaMallocManaged(&amp;arr, n * <span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">int</span>)));  <br>  <br>   kernel&lt;&lt;&lt;<span class="hljs-number">1</span>, <span class="hljs-number">4</span>&gt;&gt;&gt;(arr, n);  <br>  <br>   checkCudaErrors(cudaDeviceSynchronize());  <br>   <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; n; i++) &#123;  <br>       <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;arr[%d]: %d\n&quot;</span>, i, arr[i]);  <br>   &#125;  <br>  <br>   cudaFree(arr);<br>   <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;  <br>&#125;<br></code></pre></td></tr></table></figure>

<p>由于 cuda 的大显存设计，我们大可以将一个数组很大一块连续块装入，而且 cuda 的计算机制保证了 blockDim 不会太大（1024）。所以这里尽管是跳步加，也不会过多的伤害到空间局部性。在不同的设备上，我们可以通过直接调整 blockDim 进行性能的测试，从而调优。Debug 的时候也可以直接把 gridDim 改成 1 从而进行便捷的调试。此外，这种写法的逻辑非常好整理，也方便进行记忆。为此这种方法被人们广泛使用。</p>
<p>如果我们需要启用多个 Block, 那么我们面临一个新的问题，如果循环次数不是 blockDim 的整数倍我们如何处理？<br>这里的办法是非常国际通用的，大家广泛的使用如下写法：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-keyword">int</span> loopCount = .....;<br><br>....<br><br><span class="hljs-keyword">int</span> block_dim = ...;<br><span class="hljs-keyword">int</span> grid_dim = (loopCount - <span class="hljs-number">1</span>) / block_dim + <span class="hljs-number">1</span>;<br><br>...<br><br>call_kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;();<br></code></pre></td></tr></table></figure>

<p>好文推荐（示意图很好）： <a target="_blank" rel="noopener" href="http://alexminnaar.com/2019/08/02/grid-stride-loops.html">http://alexminnaar.com/2019/08/02/grid-stride-loops.html</a></p>
<p>官方博客中还推荐了另外一种非常灵活的写法：<br><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/cuda-pro-tip-write-flexible-kernels-grid-stride-loops/">https://developer.nvidia.com/blog/cuda-pro-tip-write-flexible-kernels-grid-stride-loops/</a></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function">__global__ <span class="hljs-keyword">void</span> <span class="hljs-title">saxpy</span><span class="hljs-params">(<span class="hljs-keyword">int</span> n, <span class="hljs-keyword">float</span> a, <span class="hljs-keyword">float</span> *x, <span class="hljs-keyword">float</span> *y)</span> </span>&#123;<br>	<span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = blockIdx.x * blockDim.x + threadIdx.x; i &lt; n; i += blockDim.x * gridDim.x) &#123;<br>		y[i] = a * x[i] + y[i];<br>	&#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<p>相当与我们的 Flatten Mode 的编程方法，确保我们不会落下数据没有循环。</p>
<p>虽说大部分 C++17 的代码 cuda 可以跑起来，但是 STL 容器 cuda 并没有很好的适配和实现，如果需要用容器，我们需要自己定义 allocator。这里推荐一个 gist： <a target="_blank" rel="noopener" href="https://gist.github.com/CommitThis/1666517de32893e5dc4c441269f1029a">https://gist.github.com/CommitThis/1666517de32893e5dc4c441269f1029a</a></p>
<p>其中定义了：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-keyword">template</span> &lt;<span class="hljs-keyword">typename</span> T&gt;<br><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">unified_alloc</span></span><br><span class="hljs-class">&#123;</span><br><span class="hljs-keyword">public</span>:<br>    <span class="hljs-keyword">using</span> value_type = T;<br>    <span class="hljs-keyword">using</span> pointer = value_type*;<br>    <span class="hljs-keyword">using</span> size_type = <span class="hljs-built_in">std</span>::<span class="hljs-keyword">size_t</span>;<br><br>    unified_alloc() <span class="hljs-keyword">noexcept</span> = <span class="hljs-keyword">default</span>;<br><br>    <span class="hljs-keyword">template</span> &lt;<span class="hljs-keyword">typename</span> U&gt;<br>    unified_alloc(unified_alloc&lt;U&gt; <span class="hljs-keyword">const</span>&amp;) <span class="hljs-keyword">noexcept</span> &#123;&#125;<br><br>    auto allocate(size_type n, const void* = 0) -&gt; value_type* &#123;<br>        value_type * tmp;<br>        <span class="hljs-keyword">auto</span> error = cudaMallocManaged((<span class="hljs-keyword">void</span>**)&amp;tmp, n * <span class="hljs-keyword">sizeof</span>(T));<br>        <span class="hljs-keyword">if</span> (error != cudaSuccess) &#123;<br>            <span class="hljs-keyword">throw</span> <span class="hljs-built_in">std</span>::runtime_error &#123; cudaGetErrorString(error) &#125;;<br>        &#125;<br>        <span class="hljs-keyword">return</span> tmp;<br>    &#125;<br><br>    auto deallocate(pointer p, size_type n) -&gt; void &#123;<br>        <span class="hljs-keyword">if</span> (p) &#123;<br>            <span class="hljs-keyword">auto</span> error = cudaFree(p);<br>            <span class="hljs-keyword">if</span> (error != cudaSuccess) &#123;<br>                <span class="hljs-keyword">throw</span> <span class="hljs-built_in">std</span>::runtime_error &#123; cudaGetErrorString(error) &#125;;<br>            &#125;<br>        &#125;<br>    &#125;<br>&#125;;<br></code></pre></td></tr></table></figure>

<p>如果此时我们需要定义 vector, 只需要：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-built_in">std</span>::<span class="hljs-built_in">vector</span>&lt;<span class="hljs-keyword">int</span>, unified_alloc&lt;<span class="hljs-keyword">int</span>&gt;&gt; arr(length);<br></code></pre></td></tr></table></figure>

<p>即可。</p>
<p>这里vector即使定义成功了，我们使用的时候还是会有一些问题，因为__global__函数的参数限制问题，我们只能传递 arr.data() 从而获取类似数组地址的方法传递参数。综上所属，楞写 STL 不是一个好办法，下面的 thrust 中会介绍一些靠谱的办法。</p>
<h3 id="4-3-2-算子传输"><a href="#4-3-2-算子传输" class="headerlink" title="4.3.2 算子传输"></a>4.3.2 算子传输</h3><p>在 cuda 编程的时候，经常遇到下面这个问题：</p>
<p>对于很多类似的函数，我们只需要更换其中的一个小函数即可，例如计算</p>
<p>$$<br>\sum_{i=0}^n Sin(a_i),;\sum_{i=0}^n Cos(a_i),;\sum_{i=0}^n Tan(a_i),;…<br>$$</p>
<p>很多时候只需要换一个算子，我们不需要重复构造很多个函数（减少函数数量其实是有一定好处的，对于模式类似的函数，如果适当利用 lambda 算子，我们可以减少编译器的编译负担）</p>
<p>这种时候我们可以采用两种方法。</p>
<p>第一种就是 lambda 算子：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs cpp">  <br><span class="hljs-keyword">template</span> &lt;<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Func</span>&gt;</span>  <br><span class="hljs-function">__global__ <span class="hljs-keyword">void</span> <span class="hljs-title">kernel</span><span class="hljs-params">(<span class="hljs-keyword">int</span> n, Func func)</span> </span>&#123;  <br>   <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = blockDim.x * blockIdx.x + threadIdx.x;  <br>        i &lt; n; i += blockDim.x * gridDim.x) &#123;  <br>       func(i);  <br>   &#125;  <br>&#125;  <br>  <br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;  <br>   <span class="hljs-keyword">int</span> n = <span class="hljs-number">10</span>;  <br>   <span class="hljs-keyword">int</span> *arr;  <br>  <br>   cudaMallocManaged(&amp;arr, n * <span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">int</span>));  <br>  <br>   <span class="hljs-keyword">int</span> block_dim = <span class="hljs-number">128</span>;  <br>   <span class="hljs-keyword">int</span> grid_dim = (n - <span class="hljs-number">1</span>) / block_dim + <span class="hljs-number">1</span>;  <br>   kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(n, [=] __device__ (<span class="hljs-keyword">int</span> i) &#123;  <br>       arr[i] = i;  <br>   &#125;);  <br>      <br>   checkCudaErrors(cudaDeviceSynchronize());  <br>   kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(n, [=] __device__ (<span class="hljs-keyword">int</span> i) &#123;  <br>       <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%d, %f\n&quot;</span>, i, sinf(arr[i]));  <br>   &#125;);  <br>  <br>   checkCudaErrors(cudaDeviceSynchronize());  <br>  <br>   <span class="hljs-comment">// Compare  </span><br>   <span class="hljs-comment">// for(int index = 0; index &lt; n; ++index) &#123;  </span><br>   <span class="hljs-comment">//    printf(&quot;%d, %f\n&quot;, index, sinf(index));  </span><br>   <span class="hljs-comment">//&#125;  </span><br>  <br>   cudaFree(arr);  <br>   <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;  <br>&#125;<br></code></pre></td></tr></table></figure>

<p>注意 lambda 算子也是要在 GPU 上执行的，所以需要加上 _<em>device_</em> 进行修饰。</p>
<p>第二种方法就是使用类似于函数指针的方式。不过 cuda 中函数指针的定义非常严格：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs cpp">  <br><span class="hljs-keyword">template</span> &lt;<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Func</span>&gt;</span>  <br><span class="hljs-function">__global__ <span class="hljs-keyword">void</span> <span class="hljs-title">kernel</span><span class="hljs-params">(<span class="hljs-keyword">int</span> *arr, <span class="hljs-keyword">int</span> n, Func func)</span> </span>&#123;  <br>   <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = blockDim.x * blockIdx.x + threadIdx.x;  <br>        i &lt; n; i += blockDim.x * gridDim.x) &#123;  <br>       func(arr, i);  <br>   &#125;  <br>&#125;  <br>  <br><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">funcop1</span> &#123;</span>  <br>   <span class="hljs-function">__device__ <span class="hljs-keyword">void</span> <span class="hljs-title">operator</span><span class="hljs-params">()</span><span class="hljs-params">(<span class="hljs-keyword">int</span> *arr, <span class="hljs-keyword">int</span> i)</span> </span>&#123;  <br>       arr[i] = i;  <br>   &#125;  <br>&#125;;  <br>  <br><span class="hljs-class"><span class="hljs-keyword">struct</span> <span class="hljs-title">funcop2</span> &#123;</span>  <br>   <span class="hljs-function">__device__ <span class="hljs-keyword">void</span> <span class="hljs-title">operator</span><span class="hljs-params">()</span><span class="hljs-params">(<span class="hljs-keyword">int</span> *arr, <span class="hljs-keyword">int</span> i)</span> </span>&#123;  <br>       <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%d %f\n&quot;</span>, arr[i], sinf(arr[i]));  <br>   &#125;  <br>&#125;;  <br>  <br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;  <br>   <span class="hljs-keyword">int</span> n = <span class="hljs-number">10</span>;  <br>   <span class="hljs-keyword">int</span> *arr;  <br>  <br>   cudaMallocManaged(&amp;arr, n * <span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">int</span>));  <br>  <br>   <span class="hljs-keyword">int</span> block_dim = <span class="hljs-number">128</span>;  <br>   <span class="hljs-keyword">int</span> grid_dim = (n - <span class="hljs-number">1</span>) / block_dim + <span class="hljs-number">1</span>;  <br>   kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(arr, n, funcop1&#123;&#125;);  <br>      <br>   checkCudaErrors(cudaDeviceSynchronize());  <br>   kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(arr, n, funcop2&#123;&#125;);  <br>  <br>   checkCudaErrors(cudaDeviceSynchronize());  <br>  <br>   <span class="hljs-comment">// Compare  </span><br>   <span class="hljs-comment">// for(int index = 0; index &lt; n; ++index) &#123;  </span><br>   <span class="hljs-comment">//    printf(&quot;%d, %f\n&quot;, index, sinf(index));  </span><br>   <span class="hljs-comment">//&#125;  </span><br>  <br>   cudaFree(arr);  <br>   <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;  <br>&#125;<br></code></pre></td></tr></table></figure>

<p>需要注意的是：</p>
<ol>
<li>由于地址类型在传递时，CPU 和 GPU 不一致，所以需要用结构体封装一层</li>
<li>结构体中的函数是在 GPU 上计算，需要用 device 修饰</li>
<li>结构体中函数需要用 operator () 修饰</li>
</ol>
<p>到这里，基础部分已经基本结束。</p>
<h2 id="4-4-高级计算行为"><a href="#4-4-高级计算行为" class="headerlink" title="4.4 高级计算行为"></a>4.4 高级计算行为</h2><h3 id="4-4-1-thrust"><a href="#4-4-1-thrust" class="headerlink" title="4.4.1 thrust"></a>4.4.1 thrust</h3><p>之前我们提及过，在 cuda 中无法使用 C++ STL 进行编程，为此我们需要使用 Nviida 提供的黑科技： thrust。</p>
<p>首先简单介绍一下，thrust 库被称为： Template library for CUDA，自从 cuda 4.0 就开始有了（甚至比 unified Memory 还早是我没想到的）。主要目的是对标 C++ STL。简化 HPC 编程。</p>
<h4 id="4-4-1-1-vector-对标"><a href="#4-4-1-1-vector-对标" class="headerlink" title="4.4.1.1 vector 对标"></a>4.4.1.1 vector 对标</h4><p>C++ STL 中 vector 容器属实是一个使用主力了，在 thrust 中我们也有对应的实现，分别为：<code>universal_vector</code>，<code>host_vector</code>，<code>device_vector</code>。他们的用途根据名字应该已经可以猜到了。使用 thrust 之后，我们可以直接使用 <code>=</code> 进行数值拷贝。Cuda 中已经完成了这一部分的重载工作。</p>
<p>下面列举一个例子：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs cpp">  <br><span class="hljs-keyword">template</span> &lt;<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Func</span>&gt;</span>  <br><span class="hljs-function">__global__ <span class="hljs-keyword">void</span> <span class="hljs-title">kernel</span><span class="hljs-params">(<span class="hljs-keyword">int</span> n, Func func)</span> </span>&#123;  <br>   <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = blockDim.x * blockIdx.x + threadIdx.x;  <br>        i &lt; n; i += blockDim.x * gridDim.x) &#123;  <br>       func(i);  <br>   &#125;  <br>&#125;  <br>  <br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;  <br>   <span class="hljs-keyword">int</span> n = <span class="hljs-number">65536</span>;  <br>  <br>   <span class="hljs-keyword">int</span> block_dim = <span class="hljs-number">128</span>;  <br>   <span class="hljs-keyword">int</span> grid_dim = (n - <span class="hljs-number">1</span>) / block_dim;  <br>      <br>   <span class="hljs-function">thrust::host_vector&lt;<span class="hljs-keyword">float</span>&gt; <span class="hljs-title">x_host</span><span class="hljs-params">(n)</span></span>;  <br>   <span class="hljs-function">thrust::host_vector&lt;<span class="hljs-keyword">float</span>&gt; <span class="hljs-title">y_host</span><span class="hljs-params">(n)</span></span>;  <br>  <br>   thrust::generate(x_host.begin(), x_host.end(), []&#123;<span class="hljs-keyword">return</span> <span class="hljs-built_in">std</span>::rand() / <span class="hljs-number">3.0</span>;&#125;);  <br>   thrust::generate(y_host.begin(), y_host.end(), []&#123;<span class="hljs-keyword">return</span> <span class="hljs-built_in">std</span>::rand() / <span class="hljs-number">11.0</span>;&#125;);  <br>  <br>   <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%f + %f = \n&quot;</span>, x_host[<span class="hljs-number">0</span>], y_host[<span class="hljs-number">0</span>]);  <br>  <br>   <span class="hljs-function">thrust::device_vector&lt;<span class="hljs-keyword">float</span>&gt; <span class="hljs-title">x_dev</span><span class="hljs-params">(n)</span></span>;  <br>   <span class="hljs-function">thrust::device_vector&lt;<span class="hljs-keyword">float</span>&gt; <span class="hljs-title">y_dev</span><span class="hljs-params">(n)</span></span>;  <br>   x_dev = x_host;  <br>   y_dev = y_host;  <br>  <br>   kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(n, [x = x_dev.data(), y = y_dev.data()] __device__ (<span class="hljs-keyword">int</span> index)&#123;  <br>       x[index] = x[index] + y[index];  <br>   &#125;);  <br>  <br>   checkCudaErrors(cudaDeviceSynchronize());  <br>   x_host = x_dev;  <br>  <br>   <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%f\n&quot;</span>, x_host[<span class="hljs-number">0</span>]);  <br>  <br>   <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;  <br>&#125;<br></code></pre></td></tr></table></figure>

<p>输出结果：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs text">601429824.000000 + 151421216.000000 =   <br>752851072.000000<br></code></pre></td></tr></table></figure>

<p>本程序成功的展示了 thrust_vector 之间互相 copy 的行为，这里辅助使用了一下 thrust :: generate 用于生成随机序列。（这就是前一节花篇幅辅助介绍算子传递的原因，lambda 算子在 thrust 中会有广泛的应用，尽管这是 C++20 才应该有的特性，但是很多编译器早早的提供了支持「赞美 LLVM」。）</p>
<h4 id="4-4-1-2-other-thrusts"><a href="#4-4-1-2-other-thrusts" class="headerlink" title="4.4.1.2 other thrusts"></a>4.4.1.2 other thrusts</h4><p>Thrust 中还有很多和 STL 对标的内容，例如 For_each、sort、count_if 这些都是非常常用的 thrust 函数，使用方法和 generate 几乎一致，没有过多差别。</p>
<p><a target="_blank" rel="noopener" href="https://thrust.github.io/doc/namespacethrust.html">https://thrust.github.io/doc/namespacethrust.html</a></p>
<p>这里个人推荐在上面的网页中查找 thrust 的 API。Doxygen 提供了详细的文档管理。我们可以查到具体的使用方法和相对应的例子。</p>
<h3 id="4-4-2-atomic"><a href="#4-4-2-atomic" class="headerlink" title="4.4.2 atomic"></a>4.4.2 atomic</h3><p>说道这个问题，很容易会和生产-消费的问题联系到一起。这里先列举一个翻车的例子：</p>
<p>(下面这个例子中顺便引入了一个 trick, 在 cuda 中我们是可以使用全局变量的，但是和我们管理内存的方式是一样的，一个全局变量也是有“位置”的，我们在 GPU 和 CPU 上的全局变量需要使用 cudaMemcpyFromSymbol 进行拷贝)</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><br>__device__ <span class="hljs-keyword">float</span> sum = <span class="hljs-number">0</span>;<br><br><span class="hljs-keyword">template</span> &lt;<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Func</span>&gt;</span><br><span class="hljs-function">__global__ <span class="hljs-keyword">void</span> <span class="hljs-title">kernel</span><span class="hljs-params">(<span class="hljs-keyword">int</span> n, Func func)</span> </span>&#123;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = blockDim.x * blockIdx.x + threadIdx.x;<br>         i &lt; n; i += blockDim.x * gridDim.x) &#123;<br>        func(i);<br>    &#125;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;<br>    <span class="hljs-keyword">int</span> n = <span class="hljs-number">65536</span>;<br>    <span class="hljs-keyword">int</span> *arr;<br>    <span class="hljs-keyword">float</span> result = <span class="hljs-number">0</span>;<br><br>    cudaMallocManaged(&amp;arr, n * <span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">int</span>));<br><br>    <span class="hljs-keyword">int</span> block_dim = <span class="hljs-number">128</span>;<br>    <span class="hljs-keyword">int</span> grid_dim = (n - <span class="hljs-number">1</span>) / block_dim;<br>    kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(n, [=] __device__ (<span class="hljs-keyword">int</span> i) &#123;<br>        arr[i] = i;<br>    &#125;);<br>    <br>    <br>    kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(n, [=] __device__ (<span class="hljs-keyword">int</span> i) &#123;<br>        sum += sinf(arr[i]);<br>    &#125;);<br><br>    cudaMemcpyFromSymbol(&amp;result, sum, <span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">float</span>), <span class="hljs-number">0</span>, cudaMemcpyDeviceToHost);<br>    checkCudaErrors(cudaDeviceSynchronize());<br>    <br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%f\n&quot;</span>, result);<br><br>    <span class="hljs-comment">// Compare</span><br>    result = <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> index = <span class="hljs-number">0</span>; index &lt; n; ++index) &#123;<br>        result += sinf(index);<br>    &#125;<br>    <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%f&quot;</span>, result);<br><br>    cudaFree(arr);<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;<br>&#125;<br></code></pre></td></tr></table></figure>

<p>上面的例子非常好理解，是我们之前一个 lambda 例子的微微改进，我们之所以说明这个例子是因为这个程序行为非常“鬼畜”。每次执行结果都可能不同。那么问题在哪里呢？</p>
<p>在于</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cpp">sum += sinf (arr[i])<br></code></pre></td></tr></table></figure>
<p>操作实际上不是原子的。我们的不同的 i 会抢占对于 sum 的使用。</p>
<p>修正方法也很简单：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><code class="hljs cpp">  <br>__device__ <span class="hljs-keyword">float</span> sum = <span class="hljs-number">0</span>;  <br>  <br><span class="hljs-keyword">template</span> &lt;<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Func</span>&gt;</span>  <br><span class="hljs-function">__global__ <span class="hljs-keyword">void</span> <span class="hljs-title">kernel</span><span class="hljs-params">(<span class="hljs-keyword">int</span> n, Func func)</span> </span>&#123;  <br>   <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = blockDim.x * blockIdx.x + threadIdx.x;  <br>        i &lt; n; i += blockDim.x * gridDim.x) &#123;  <br>       func(i);  <br>   &#125;  <br>&#125;  <br>  <br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;  <br>   <span class="hljs-keyword">int</span> n = <span class="hljs-number">65536</span>;  <br>   <span class="hljs-keyword">int</span> *arr;  <br>   <span class="hljs-keyword">float</span> result = <span class="hljs-number">0</span>;  <br>  <br>   cudaMallocManaged(&amp;arr, n * <span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">int</span>));  <br>  <br>   <span class="hljs-keyword">int</span> block_dim = <span class="hljs-number">128</span>;  <br>   <span class="hljs-keyword">int</span> grid_dim = (n - <span class="hljs-number">1</span>) / block_dim;  <br>   kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(n, [=] __device__ (<span class="hljs-keyword">int</span> i) &#123;  <br>       arr[i] = i;  <br>   &#125;);  <br>      <br>      <br>   kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(n, [=] __device__ (<span class="hljs-keyword">int</span> i) &#123;  <br>       atomicAdd(&amp;sum, sinf(arr[i]));  <br>   &#125;);  <br>  <br>   cudaMemcpyFromSymbol(&amp;result, sum, <span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">float</span>), <span class="hljs-number">0</span>, cudaMemcpyDeviceToHost);  <br>   checkCudaErrors(cudaDeviceSynchronize());  <br>      <br>   <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%f\n&quot;</span>, result);  <br>  <br>   <span class="hljs-comment">// Compare  </span><br>   result = <span class="hljs-number">0</span>;  <br>   <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> index = <span class="hljs-number">0</span>; index &lt; n; ++index) &#123;  <br>       result += sinf(index);  <br>   &#125;  <br>   <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%f\n&quot;</span>, result);  <br>  <br>   cudaFree(arr);  <br>   <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;  <br>&#125;<br></code></pre></td></tr></table></figure>

<p>即改为：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cpp">atomicAdd(&amp;sum, sinf(arr[i]));  <br></code></pre></td></tr></table></figure>

<p>此时就只有一点点由于加法顺序不同导致的精度误差了。这一些误差是可以被接受的。</p>
<p>那么我们继续分析一下这个问题，在 cuda 内部我们是如何实现原子的。这个问题在“编译原理”课程中可能有过类似的解答。<br>为了实现一个操作的原子性，我们需要“盯着他的数值”，只有我们“计算完成”“原始值也没变化”的时候，我们才能成功的优化。</p>
<p>换而言之，类似 atomicAdd, 我们又如下的步骤：</p>
<ul>
<li>Value_prev = Target</li>
<li>Target += …</li>
<li>Return Value_prev</li>
</ul>
<p>如果此时 Value_prev 和现在的状态一致的，就可以进行更新。</p>
<p>Cuda 中还有这些原子函数：</p>
<p>atomicAdd (dst, src)<br>atomicSub(dst, src)<br>atomicOr(dst, src)<br>atomicAnd(dst, src)<br>atomicXor(dst, src)<br>atomicMax(dst, src)<br>atomicMin(dst, src)</p>
<p>他们都有返回值，返回违背更改前的数值。</p>
<p>更加一般的，我们可以自己定义一个属于自己的原子操作：</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs cpp">  <br>__device__ <span class="hljs-keyword">float</span> sum = <span class="hljs-number">0</span>;  <br>  <br><span class="hljs-function">__device__ <span class="hljs-keyword">float</span> <span class="hljs-title">my_atom_add</span><span class="hljs-params">(<span class="hljs-keyword">float</span> *dst, <span class="hljs-keyword">float</span> src)</span></span>&#123;  <br>   <span class="hljs-keyword">int</span> old = __float_as_int(*dst);  <br>   <span class="hljs-keyword">int</span> expect;  <br>   <span class="hljs-keyword">do</span> &#123;  <br>       expect = old;  <br>       old = atomicCAS((<span class="hljs-keyword">int</span> *)dst, expect,  <br>               __float_as_int(__int_as_float(expect) + sinf(src)));  <br>   &#125; <span class="hljs-keyword">while</span>(expect != old);  <br>   <span class="hljs-keyword">return</span> old;  <br>&#125;  <br>  <br><span class="hljs-keyword">template</span> &lt;<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Func</span>&gt;</span>  <br><span class="hljs-function">__global__ <span class="hljs-keyword">void</span> <span class="hljs-title">kernel</span><span class="hljs-params">(<span class="hljs-keyword">int</span> n, Func func)</span> </span>&#123;  <br>   <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = blockDim.x * blockIdx.x + threadIdx.x;  <br>        i &lt; n; i += blockDim.x * gridDim.x) &#123;  <br>       func(i);  <br>   &#125;  <br>&#125;  <br>  <br><span class="hljs-function"><span class="hljs-keyword">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span> </span>&#123;  <br>   <span class="hljs-keyword">int</span> n = <span class="hljs-number">65536</span>;  <br>   <span class="hljs-keyword">int</span> *arr;  <br>   <span class="hljs-keyword">float</span> result = <span class="hljs-number">0</span>;  <br>  <br>   cudaMallocManaged(&amp;arr, n * <span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">int</span>));  <br>  <br>   <span class="hljs-keyword">int</span> block_dim = <span class="hljs-number">128</span>;  <br>   <span class="hljs-keyword">int</span> grid_dim = (n - <span class="hljs-number">1</span>) / block_dim;  <br>   kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(n, [=] __device__ (<span class="hljs-keyword">int</span> i) &#123;  <br>       arr[i] = i;  <br>   &#125;);  <br>      <br>      <br>   kernel&lt;&lt;&lt;grid_dim, block_dim&gt;&gt;&gt;(n, [=] __device__ (<span class="hljs-keyword">int</span> i) &#123;  <br>       my_atom_add(&amp;sum, arr[i]);  <br>   &#125;);  <br>  <br>   cudaMemcpyFromSymbol(&amp;result, sum, <span class="hljs-keyword">sizeof</span>(<span class="hljs-keyword">float</span>), <span class="hljs-number">0</span>, cudaMemcpyDeviceToHost);  <br>   checkCudaErrors(cudaDeviceSynchronize());  <br>      <br>   <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%f\n&quot;</span>, result);  <br>  <br>   <span class="hljs-comment">// Compare  </span><br>   result = <span class="hljs-number">0</span>;  <br>   <span class="hljs-keyword">for</span>(<span class="hljs-keyword">int</span> index = <span class="hljs-number">0</span>; index &lt; n; ++index) &#123;  <br>       result += sinf(index);  <br>   &#125;  <br>   <span class="hljs-built_in">printf</span>(<span class="hljs-string">&quot;%f\n&quot;</span>, result);  <br>  <br>   cudaFree(arr);  <br>   <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>;  <br>&#125;<br></code></pre></td></tr></table></figure>

<p>我们使用 atomoicCAS 按照类似的逻辑步骤：</p>
<ul>
<li>记录维护原始值</li>
<li>试图 CAS 更改</li>
<li>成功改动之后可以停下</li>
</ul>
<p>这里使用了一些技巧，结合下面的官网借口解释：</p>
<p><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#atomiccas">https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#atomiccas</a></p>
<img src="image5.png" srcset="/img/loading.gif" lazyload width="80%" height="80%">

<p>AtomicCAS 是一个只支持整数的借口，如果需要浮点支持，需要进行 as float 进行转换。</p>
<p>这里不难看出另一个问题，如果我们的原子操作被严格执行，那么原子操作会成为一个严重的瓶颈。相当于所有的数据都要过一遍着一个原子操作。但是实际上GPU跑起来还是非常快的，这是因为GPU根据blockDim和gridDim进行了操作，部分串行。以求和为例，我们会将数据分成几个大块，分别计算部分和，最后再进行规约。利用这种方法保证了此处不会成为瓶颈。<br>因此也不难看出：原子操作都是可以 Fork-And-Merge 的操作。</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Skill/" class="category-chain-item">Skill</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/Cuda/">#Cuda</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>2202-CudaProgramming</div>
      <div>https://chivier.github.io/2022/02/20/2022/2202-CudaProgramming/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>Chivier Humber</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>February 20, 2022</div>
        </div>
      
      
      <div class="license-meta-item">
        <div>Licensed under</div>
        <div>
          
            
            
              <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
              <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                <i class="iconfont icon-by"></i>
              </span>
              </a>
            
          
        </div>
      </div>
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/03/09/2022/2203-TFcustom/" title="2203-TFcustom">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">2203-TFcustom</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/02/04/2022/2202-QemuTest/" title="2202-QemuTest">
                        <span class="hidden-mobile">2202-QemuTest</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;Table of Contents</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      headingSelector : CONFIG.toc.headingSelector || 'h1,h2,h3,h4,h5,h6',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      collapseDepth   : CONFIG.toc.collapseDepth || 0,
      scrollSmooth    : true,
      headingsOffset  : -boardTop
    });
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.10/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>


</body>
</html>
